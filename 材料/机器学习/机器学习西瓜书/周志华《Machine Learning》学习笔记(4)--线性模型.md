笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义/术语、学习器性能的评估/度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容--线性模型。

#**3、线性模型**

谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y=ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。

![1.png](https://i.loli.net/2018/10/17/5bc722b068e48.png)


##**3.1 线性回归**

线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000-->13亿...2016-->15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）-->15k。

有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：

- 若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。

- 若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。

线性回归试图学的：
$$f(x_i)=wx_i+b,使得f(x_i)\sim y_i$$
如何确定 $w$ 和 $b$ 呢？显然，问题的关键在于如何衡量 $f(x)$ 与 $y$ 之间的差别。一般情况下，我们可以考虑使用均方误差最小化的方法。
$$(w^*,b^*)=argmin_{w,b}\sum_{i=1}^m(f(x_i)-y_i)^2$$
均方误差（Mean Square Error）有非常好的几何意义，它对应了常用的欧几里得距离或简称“欧氏距离”（Euclidean distance）。基于均方误差最小化进行模型求解的方法称为“最小二乘法”（least square method）。在线性回归中，最小二乘法就是试图找到一条直线，使所有样本到直线上的欧式距离之和最小。

通过求解$w$ 和 $b$ 使得模型均方误差最小化的过程称为线性回归模型的最小二乘“参数估计”（parameter estimation）。通过将损失函数分别对 $w$ 和 $b$ 进行求导，并令其等于0，可得两者最优解的封闭解（closed-form）

> 凸函数：对于区间$[a,b]$上定义的函数 $f$ ,若它对区间中任意两点$x_1,x_2$均有$f(\frac{x_1+x_2}{2})\le\frac{f(x_1)+f(x_2)}{2}$,则称函数 $f$ 为区间 $[a,b]$ 上的凸函数。

> U形曲线的函数如$f=x^2$，通常是凸函数。

> 对实数集上的函数，可通过求二阶导数来判别：若二阶导数在区间上非负，则称为凸函数；若二阶导数在区间上恒大于0，则称为严格凸函数。


（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y=wx+b的两个参数w和b，计算过程如下图所示：

![2.png](https://i.loli.net/2018/10/17/5bc722b0ccec4.png)

（2）多元线性回归：当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2...xd）,y}，则y=wx+b需要写成：

![0.png](https://i.loli.net/2018/10/17/5bc72567b8bcd.png)

通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：

![3.png](https://i.loli.net/2018/10/17/5bc722b0ad8f7.png)

![4.png](https://i.loli.net/2018/10/17/5bc722b0af652.png)

![5.png](https://i.loli.net/2018/10/17/5bc722b090543.png)

同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化。

![6.png](https://i.loli.net/2018/10/17/5bc722b0cde33.png)

然而现实中，$X^TX$往往不是满秩矩阵，例如在许多任务中我们会遇到大量的变量，其数目甚至超过样例数，此时可解出多个$\hat{w}$,它们都能使均方误差最小化，选择哪一个解作为输出，将由学习算法的归纳偏好决定，常见的做法是引入正则化（Regularization）项。


另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：

![7.png](https://i.loli.net/2018/10/17/5bc722b103cbf.png)

此时模型被称为“对数线性回归”（log-linear regression），它实际上是在试图让 $e^{w^Tx+b}$逼近$y$。
上式在形式上仍是线性回归，当实质上已是在求取输入空间到输出空间的非线性函数映射，这里的对数函数起到了将线性回归模型的预测值与真实标记联系起来的作用。

更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）（连续且充分光滑）。

![8.png](https://i.loli.net/2018/10/17/5bc722b0a2841.png)

> 广义线性模型的参数估计常通过加权最小二乘法或极大似然法进行。

##**3.2 线性几率回归**

回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。

如果考虑二分类任务，我们需要将模型的输出值转换为0/1值，最理想的就是“单位阶跃函数”（unit-step function，亦称Heaviside函数）。
$$ y=\begin{cases} 0, & z<0 \\ 0.5, & z=0  \\ 1,&z>0\end{cases}$$
然而，单位阶跃函数不连续，无法直接用作联系函数。因此我们考虑使用对数几率函数（logistic function）。

> sigmoid 函数即形似S的函数，对率函数是Sigmoid函数最重要的代表。

对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。

![9.png](https://i.loli.net/2018/10/17/5bc722b0c7748.png)

![10.png](https://i.loli.net/2018/10/17/5bc722b0a655d.png)

若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。

>特别注意，虽然它的名字是“回归”，但实际确实一种分类学习方法。这种方法有很多优点，例如
>1）它是直接对分类可能性进行建模，无需事先假设数据分布，这样就避免了假设分布不准确所带来的的问题
>2）它不是仅预测出“类别”，而是可得到近似概率预测，这对许多需要利用概率辅助决策的任务很有用
>3）此外，逻辑回归的目标函数是任意阶可导地凸函数，易于求解。

下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。

![11.png](https://i.loli.net/2018/10/17/5bc723b824f0c.png)

![12.png](https://i.loli.net/2018/10/17/5bc723b817961.png)



##**3.3 线性判别分析**

线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：

![13.png](https://i.loli.net/2018/10/17/5bc723b863ebb.png)![14.png](https://i.loli.net/2018/10/17/5bc723b85bfa9.png)

想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不同类之间中心的距离尽可能大。最大化目标如下：
$$\begin{aligned}
J&=\frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\sum_0w+w^T\sum_1w}\\
&=\frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\sum_0+\sum_1)w}
\end{aligned}
$$
基于这样的考虑，LDA定义了两个散度矩阵。

+ 类内散度矩阵（within-class scatter matrix）

![15.png](https://i.loli.net/2018/10/17/5bc723b8156e1.png)

+ 类间散度矩阵(between-class scaltter matrix)

![16.png](https://i.loli.net/2018/10/17/5bc723b7e9db3.png)

因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。

![17.png](https://i.loli.net/2018/10/17/5bc723b7e8a61.png)

从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。

![18.png](https://i.loli.net/2018/10/17/5bc723b83d5e0.png)

第4行：$(u_0-u_1)^Tw$ 为标量。

考虑数值稳定性，$S_w^{-1}$ 通常采用奇异值分解的方式求解。

值得一提，LDA可从贝叶斯决策理论的角度来阐释，并可证明，当两类数据同先验、满足高斯分布且协方差相等时，LDA可达到最优分类。

若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。    
​             
##**3.4 多分类学习**

现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。

+ OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类/一个反类），从而产生N（N-1）/2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。

+ OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。

+ MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明/欧式距离选择距离最小的类别作为最终分类结果。

![19.png](https://i.loli.net/2018/10/17/5bc723b862bfb.png)

![20.png](https://i.loli.net/2018/10/17/5bc723b8300d5.png)

> ECOC为何被称为纠错输出码：因为在测试阶段，ECOC编码对分类器的错误具有一定的容忍和修正能力。也就是说，当存在某个分类器产生错误，当仍能够产生正确的分类结果。一般来说，对同一个学习任务，ECOC编码越长，纠错能力就越强。对于同等长度的编码而言，理论上来说，任意两个类别之间的编码距离越远，则纠错能力越强。


##**3.5 类别不平衡问题**

前面介绍的分类学习方法都有一个共同的基本假设，即不同类别的训练样本数目相当。

类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：

1.  在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。
2.  在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。
3.  直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。（阈值移动（threshold-moving））
![21.png](https://i.loli.net/2018/10/17/5bc726fe87ae2.png)









