写在前面的话：距离上篇博客竟过去快一个月了，写完神经网络博客正式进入考试模式，几次考试+几篇报告下来弄得心颇不宁静了，今日定下来看到一句鸡血：Tomorrow is another due！也许生活就需要一些deadline~~

上篇主要介绍了神经网络。首先从生物学神经元出发，引出了它的数学抽象模型--MP神经元以及由两层神经元组成的感知机模型，并基于梯度下降的方法描述了感知机模型的权值调整规则。由于简单的感知机不能处理线性不可分的情形，因此接着引入了含隐层的前馈型神经网络，BP神经网络则是其中最为成功的一种学习方法，它使用误差逆传播的方法来逐层调节连接权。最后简单介绍了局部/全局最小以及目前十分火热的深度学习的概念。本篇围绕的核心则是曾经一度取代过神经网络的另一种监督学习算法--**支持向量机**（Support Vector Machine），简称**SVM**。

# **6、支持向量机**

支持向量机是一种经典的二分类模型，基本模型定义为特征空间中最大间隔的线性分类器，其学习的优化目标便是间隔最大化，因此支持向量机本身可以转化为一个凸二次规划求解的问题。

## **6.1 函数间隔与几何间隔**

对于二分类学习，假设现在的数据是线性可分的，这时分类学习最基本的想法就是找到一个合适的超平面，该超平面能够将不同类别的样本分开，类似二维平面使用ax+by+c=0来表示，超平面实际上表示的就是高维的平面，如下图所示：

![1.png](https://i.loli.net/2018/10/17/5bc72f6a2ec8a.png)

对数据点进行划分时，易知：当超平面距离与它最近的数据点的间隔越大，分类的鲁棒性越好，即当新的数据点加入时，超平面对这些点的适应性最强，出错的可能性最小。因此需要让所选择的超平面能够最大化这个间隔Gap（如下图所示）， 常用的间隔定义有两种，一种称之为函数间隔，一种为几何间隔，下面将分别介绍这两种间隔，并对SVM为什么会选用几何间隔做了一些阐述。

![2.png](https://i.loli.net/2018/10/17/5bc72f6a06d5a.png)

### **6.1.1 函数间隔**

在超平面w'x+b=0确定的情况下，|w'x*+b|能够代表点x*距离超平面的远近，易知：当w'x*+b>0时，表示x*在超平面的一侧（正类，类标为1），而当w'x*+b<0时，则表示x*在超平面的另外一侧（负类，类别为-1），因此（w'x*+b）y* 的正负性恰能表示数据点x*是否被分类正确。于是便引出了**函数间隔**的定义（functional margin）:

![3.png](https://i.loli.net/2018/10/17/5bc72f690a14b.png)

而超平面（w,b）关于所有样本点（Xi，Yi）的函数间隔最小值则为超平面在训练数据集T上的函数间隔：

![4.png](https://i.loli.net/2018/10/17/5bc72f690ac26.png)

可以看出：这样定义的函数间隔在处理SVM上会有问题，当超平面的两个参数w和b同比例改变时，函数间隔也会跟着改变，但是实际上超平面还是原来的超平面，并没有变化。例如：w1x1+w2x2+w3x3+b=0其实等价于2w1x1+2w2x2+2w3x3+2b=0，但计算的函数间隔却翻了一倍。从而引出了能真正度量点到超平面距离的概念--几何间隔（geometrical margin）。

###** 6.1.2 几何间隔**

**几何间隔**代表的则是数据点到超平面的真实距离，对于超平面w'x+b=0，w代表的是该超平面的法向量，设x*为超平面外一点x在法向量w方向上的投影点，x与超平面的距离为r，则有x*=x-r(w/||w||)，又x*在超平面上，即w'x*+b=0，代入即可得：

![5.png](https://i.loli.net/2018/10/17/5bc72f697d499.png)

为了得到r的绝对值，令r呈上其对应的类别y，即可得到几何间隔的定义：

![6.png](https://i.loli.net/2018/10/17/5bc72f696fd10.png)

从上述函数间隔与几何间隔的定义可以看出：实质上函数间隔就是|w'x+b|，而几何间隔就是点到超平面的距离。

> 样本空间中任意点x 到超平面(w,b)的距离可以写为：
> $$r=\frac{|w^Tx+b|}{||w||}$$


## **6.2 最大间隔与支持向量**

假设超平面（w,b）能将训练样本正确分类，即对于$(x_i,y_i)\in D$，若$y_i=+1$，则有$w^Tx_i+b>0$；若$y_i=-1$，则有$w^Tx_i+b<0$。令：

![[Pasted image 20221017095445.png]]

如图6.2 所示，距离超平面最近的这几个训练样本使式（6.2）的等号成立，它们被称为“支持向量”（support vector）,两个异类支持向量到超平面的距离之和为：
$$\gamma=\frac{2}{||w||}$$
它被称为“间隔”（margin）。

![[Pasted image 20221017095459.png]]

欲找到具有“最大间隔”（maximum margin）的划分超平面，也就是要找到能满足6.3中约束的参数w和b，使得$\gamma$ 最大，即：

![[Pasted image 20221017095910.png]]

其等价于：

![[Pasted image 20221017095925.png]]

这就是支持向量机（Support Vector Machine，SVM）的基本思想。


## **6.3 从原始优化问题到对偶问题**

### 6.3.1 对偶问题

我们希望求解式（6.6）来得到大间隔划分超平面所对应的模型：
$$f(x)=w^Tx+b\tag{6.7}$$
其中，w和b是模型参数。我们可以发现式（6.6）本身是一个带约束的凸二次规划问题（convex quadratic programming）。按书上所说可以使用现成的优化计算包（QP优化包）求解，但由于SVM的特殊性，一般我们将原问题变换为它的**对偶问题**，接着再对其对偶问题进行求解。为什么通过对偶问题进行求解，有下面两个原因：

	* 一是因为使用对偶问题更容易求解；
	* 二是因为通过对偶问题求解出现了向量内积的形式，从而能更加自然地引出核函数。

对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对于当前的优化问题，首先我们写出它的朗格朗日函数：

![11.png](https://i.loli.net/2018/10/17/5bc72f9332be7.png)

上式很容易验证：当其中有一个约束条件不满足时，L的最大值为 ∞（只需令其对应的α为 ∞即可）；当所有约束条件都满足时，L的最大值为1/2||w||^2（此时令所有的α为0），因此实际上原问题等价于：

![12.png](https://i.loli.net/2018/10/17/5bc72f93321c5.png)

由于这个的求解问题不好做，因此一般我们将最小和最大的位置交换一下（需满足KKT条件） ，变成原问题的对偶问题：

![13.png](https://i.loli.net/2018/10/17/5bc72f9330967.png)

这样就将原问题的求最小变成了对偶问题求最大（用对偶这个词还是很形象），接下来便可以先求L对w和b的极小，再求L对α的极大。

（1）首先求L对w和b的极小，分别求L关于w和b的偏导，可以得出：

![14.png](https://i.loli.net/2018/10/17/5bc72f9333e66.png)

将上述结果代入L得到：

![15.png](https://i.loli.net/2018/10/17/5bc72f935ae21.png)

（2）接着L关于α极大求解α（通过SMO算法求解，此处不做深入）。

![16.png](https://i.loli.net/2018/10/17/5bc72f9338a9d.png)

（3）最后便可以根据求解出的α，计算出w和b，从而得到分类超平面函数。

![17.png](https://i.loli.net/2018/10/17/5bc72f93419ca.png)

> KKT 条件：
> ![[Pasted image 20221017100830.png]]
> 

于是，对任意训练样本$(x_i,y_i)$，总有$\alpha_i=0$或$y_if(x_i)=1$，若$\alpha_i=0$，则该样本将不会在求和式中出现，也就不会对$f(x)$有任何影响；若$\alpha_i>0$，则必有$y_if(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。

在对新的点进行预测时，实际上就是将数据点x*代入分类函数f(x)=w'x+b中，若f(x)>0，则为正类，f(x)<0，则为负类，根据前面推导得出的w与b，分类函数如下所示，此时便出现了上面所提到的内积形式。

![18.png](https://i.loli.net/2018/10/17/5bc72f9353166.png)

这里实际上只需计算新样本与支持向量的内积，因为对于非支持向量的数据点，其对应的拉格朗日乘子一定为0，根据最优化理论（K-T条件），对于不等式约束y(w'x+b)-1≥0，满足：

![19.png](https://i.loli.net/2018/10/17/5bc72f933c947.png)        

> SMO算法补充：
> 上述问题可以所使用的通用的二次规划，但是该问题的规模正比于训练样本数，这会在实际任务中造成很大的开销。SMO就是利用了问题本身的特性，避开了上述问题，从而进行高效求解的算法。
> SMO的基本思路是先固定$\alpha_i$之外的所有参数，然后求$\alpha_i$ 上的极值。由于存在约束 $\sum_{i=1}^m\alpha_iy_i=0$,若固定$\alpha_i$之外的其他变量，则$\alpha_i$可由其他变量导出。于是，SMO每次选择两个变量 $\alpha_i$ 和 $\alpha_j$，并固定其他参数。这样，在参数初始化后，SMO不断执行如下两个步骤直至收敛;
> - 选取一对需要更新的变量 $\alpha_i$ 和 $\alpha_j$;
> - 固定$\alpha_i$ 和 $\alpha_j$以外的参数，进行求解，已获得更新后的$\alpha_i$ 和 $\alpha_j$;
> ![[Pasted image 20221017103947.png]]


## **6.4 核函数**

由于上述的超平面只能解决线性可分的问题，对于线性不可分的问题，例如：异或问题，我们需要使用核函数将其进行推广。一般地，解决线性不可分问题时，常常采用**映射**的方式，将低维原始空间映射到高维特征空间，使得数据集在高维空间中变得线性可分，从而再使用线性学习器分类。如果原始空间为有限维，即属性数有限，那么总是存在一个高维特征空间使得样本线性可分，如下图6.3所示。

![[Pasted image 20221018083136.png]]


若∅（x）代表将x映射后的特征向量，则在特征空间中划分超平面所对应的模型可表示为：

![20.png](https://i.loli.net/2018/10/17/5bc72f934303e.png)

按照同样的方法，先写出新目标函数的拉格朗日函数，接着写出其对偶问题，求L关于w和b的极大，最后运用SOM求解α。可以得出：

（1）原对偶问题变为：

![21.png](https://i.loli.net/2018/10/17/5bc730cc68b3b.png)

（2）原分类函数变为：
​    ![22.png](https://i.loli.net/2018/10/17/5bc730cc1b673.png)

求解的过程中，只涉及到了高维特征空间中的内积运算，由于特征空间的维数可能会非常大，例如：若原始空间为二维，映射后的特征空间为5维，若原始空间为三维，映射后的特征空间将是19维，之后甚至可能出现无穷维，根本无法进行内积运算了，此时便引出了**核函数**（Kernel）的概念。

![23.png](https://i.loli.net/2018/10/17/5bc730cc49adc.png)

因此，核函数可以直接计算隐式映射到高维特征空间后的向量内积，而不需要显式地写出映射后的结果，它虽然完成了将特征从低维到高维的转换，但最终却是在低维空间中完成向量内积计算，与高维特征空间中的计算等效**（低维计算，高维表现）**，从而避免了直接在高维空间无法计算的问题。引入核函数后，原来的对偶问题与分类函数则变为：

（1）对偶问题：

![24.png](https://i.loli.net/2018/10/17/5bc730cc173b2.png)

（2）分类函数：

![25.png](https://i.loli.net/2018/10/17/5bc730cc05959.png)

上式也被称为“支持向量展式”（support vector expansion）

因此，在线性不可分问题中，核函数的选择成了支持向量机的最大变数，若选择了不合适的核函数，则意味着将样本映射到了一个不合适的特征空间，则极可能导致性能不佳。同时，核函数需要满足以下这个必要条件：

![26.png](https://i.loli.net/2018/10/17/5bc730ccc468c.png)

由上述定理可知，只要一个对称函数所对应的核矩阵半正定，它就能作为核函数使用，事实上，对于一个半正定核矩阵，总能找到一个与之对应的映射$\Phi$,换言之，任何一个核函数都隐式的定义了一个称为“再生核希尔伯特空间”（Reproducing Kernel Hilbert Space,RKHS）的特征空间。

由于核函数的构造十分困难，通常我们都是从一些常用的核函数中选择，下面列出了几种常用的核函数：

![27.png](https://i.loli.net/2018/10/17/5bc730ccc541a.png)

此外，还可以通过函数组合得到，例如：
- 若 $k_1$ 和 $k_2$ 为核函数，则对于任意正数$\gamma_1,\gamma_2$,其线性组合：
$$\gamma_1k_1+\gamma_2k_2$$
也是核函数
- 若$k_1$ 和 $k_2$ 为核函数，则核函数的直积：
![[Pasted image 20221018085909.png]]
也是核函数；
- 若$k_1$为核函数，则对于任意函数$g(x)$,
$$k(x,z)=g(x)k_1(x,z)g(z)$$
也是核函数

## **6.5 软间隔支持向量机**

前面的讨论中，我们主要解决了两个问题：当数据线性可分时，直接使用最大间隔的超平面划分；当数据线性不可分时，则通过核函数将数据映射到高维特征空间，使之线性可分。然而在现实问题中，对于某些情形还是很难处理，例如数据中有**噪声**的情形，噪声数据（**outlier**）本身就偏离了正常位置，但是在前面的SVM模型中，我们要求所有的样本数据都必须满足约束，如果不要这些噪声数据还好，当加入这些outlier后导致划分超平面被挤歪了，如下图所示，对支持向量机的泛化性能造成很大的影响。

![28.png](https://i.loli.net/2018/10/17/5bc730ccce68e.png)

为了解决这一问题，我们需要允许某一些数据点不满足约束，即可以在一定程度上偏移超平面，同时使得不满足约束的数据点尽可能少，这便引出了**“软间隔”支持向量机**的概念

	* 允许某些数据点不满足约束y(w'x+b)≥1；
	* 同时又使得不满足约束的样本尽可能少。

这样优化目标变为：

![29.png](https://i.loli.net/2018/10/17/5bc730cc6c9fe.png)

上式中 $C>0$ 是一个常数。
显然，当$C$为无穷大时，上式迫使所有样本均满足约束，也就等价于硬间隔支持向量机，而当$C$取有限值时，上式允许一些样本不满足约束。

如同阶跃函数，0/1损失函数虽然表示效果最好，但是数学性质不佳。因此常用其它函数作为“替代损失函数”。

![30.png](https://i.loli.net/2018/10/17/5bc730cc5e5a9.png)

![[Pasted image 20221018091149.png]]

支持向量机中的损失函数为**hinge损失**，引入“**松弛变量”**，目标函数与约束条件可以写为：

![31.png](https://i.loli.net/2018/10/17/5bc7317aa3411.png)

这就是常用的“软间隔支持向量机”。

其中C为一个参数，控制着目标函数与新引入正则项之间的权重，这样显然每个样本数据都有一个对应的松弛变量，用以表示该样本不满足约束的程度，将新的目标函数转化为拉格朗日函数得到：

![32.png](https://i.loli.net/2018/10/17/5bc7317a4c96e.png)

其中$\alpha_i\ge0$，$\mu\ge0$是拉格朗日乘子。

按照与之前相同的方法，先让L求关于w，b以及松弛变量的极小，再使用SMO求出α，有：

![33.png](https://i.loli.net/2018/10/17/5bc7317a6dff2.png)

将w代入L化简，便得到其对偶问题：

![34.png](https://i.loli.net/2018/10/17/5bc7317ab6646.png)

将“软间隔”下产生的对偶问题与原对偶问题对比可以发现：新的对偶问题只是约束条件中的α多出了一个上限C，其它的完全相同，因此在引入核函数处理线性不可分问题时，便能使用与“硬间隔”支持向量机完全相同的方法。

类似的对软间隔支持向量机，KKT条件要求：

![[Pasted image 20221018092442.png]]

![[Pasted image 20221018092810.png]]

那么能否使用其他的替代损失函数呢？

![[Pasted image 20221018093058.png]]

![[Pasted image 20221018093459.png]]

## 6.6 支持向量回归

在此处，我们考虑回归问题，给定训练样本$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},y_i\in R$,希望学的一个形如6.7的回归模型，使得$f(x)$与$y$尽可能接近，$w$和$b$是待确定的模型参数。

对样本（x,y），传统的回归模型通常直接基于模型输出$f(x)$与真实输出$y$之间的差别来计算损失，当且仅当$f(x)$与$y$ 完全相同时，损失才为零。于此不同，支持向量回归（Support Vector Regression,SVR）假设我们能容忍$f(x)$与$y$之间最多有$\epsilon$的偏差，即仅当$f(x)$与$y$之间的差别绝对值大于$\epsilon$时才计算损失。如下图所示，这相当于以$f(x)$为中心，构建了一个宽度$2\epsilon$的间隔带，若训练样本落入此间隔带，则认为是被预测正确的。

![[Pasted image 20221018103005.png]]

于是，SVR问题可形式化为：
$$min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^m\mathcal{l}_{\epsilon}(f(x_i)-y_i)$$
其中$C$为正则化常数，$\mathcal{l}_{\epsilon}$是图6.7所示的$\epsilon$-不敏感损失（$\epsilon$-insensitive loss）函数。

![[Pasted image 20221018104839.png]]

引入松弛变量$\xi_i$ 和 $\hat{\xi_i}$可将上式重写为：

![[Pasted image 20221018105150.png]]

![[Pasted image 20221018105204.png]]
![[Pasted image 20221018105239.png]]

类似的，通过引入拉格朗日乘子的方式，可得拉格朗日函数

![[Pasted image 20221018105351.png]]

类似于之前的方法，令L对w,b,$\xi_i$ 和 $\hat{\xi_i}$分别求偏导，并令偏导为零可得：

![[Pasted image 20221018140532.png]]

将上述式子带入原拉格朗日函数中，即可得到SVR的对偶问题：

![[Pasted image 20221018140722.png]]

上述过程中需满足KKT条件，即要求

![[Pasted image 20221018140836.png]]

![[Pasted image 20221018145627.png]]

通过将6.47带入6.7中，可得SVR的解形如：
$$f(x)=\sum_{i=1}^m(\hat{\alpha_i}-\alpha_i)x_i^Tx+b\tag{6.53}$$
能使上式中的$(\hat{\alpha_i}-\alpha_i)\neq 0$的样本即为SVR的支持向量，它们必落在$\epsilon$-间隔带之外。显然，SVR的支持向量仅是训练样本的一部分，即其解仍具有稀疏性。

![[Pasted image 20221018150448.png]]

若考虑特征映射形式，则相应的式6.47将形如
$$w=\sum_{i=1}^m(\hat{alpha_i}-\alpha_i)\phi(x_i)\tag{6.55}$$
将式6.55带入6.19，则SVR可表示为：
$$f(x)=\sum_{i=1}^m(\hat{\alpha_i}-\alpha_i)k(x,x_i)+b,\tag{6.56}$$
其中 $k(x_i,x_j)=\phi(x_i)^T\phi(x_j)$ 为核函数。

## 6.7 核方法

回顾之前的SVM和SVR，不难发现，给定训练样本$\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\}$,若不考虑偏移项$b$，则无论SVM还是SVR，学得的模型总能表示成核函数$k(x,x_i)$的线性组合，不仅如此，事实上，我们有下面这个称为“表示定理”的更一般的结论。

![[Pasted image 20221018154854.png]]

人们发展出一系列基于核函数的学习方法，统称为“核方法”（kernel methods）。最常见的，是通过“核化”（即引入核函数）来将线性学习器拓展为非线性学习器。下面我们以线性判别分析为例来演示如何通过核化来对其进行非线性扩展，从而得到“核线性判别分析”（Kernelized Linear Discriminant Analysis，KLDA）。

我们先假设可通过某种映射$\phi$：$\mathcal{X}->F$将样本映射到一个特征空间$F$,然后在F中执行线性判别分析，以求得：
$$h(x)=w^T\phi(x)\tag{6.59}$$
类似于之前的LDA，KLDA的学习目标是：

![[Pasted image 20221018155727.png]]

通常我们难以知道映射$\phi$的具体形式，因此使用核函数$k(x,x_i)=\phi(x_i)^T\phi(x)$来隐式地表达这个映射和特征空间F。把$J(w)$作为式（6.57）中的损失函数$l$,再令$\Omega=0$,由表示定理，函数$h(x)$可写为：

![[Pasted image 20221018160334.png]]

于是，由式6.59可得：

![[Pasted image 20221018160823.png]]

![[Pasted image 20221018160931.png]]

显然，使用线性判别分析求解方法即可得到$\alpha$，进而可由式6.64得到投影函数$h(x)$。




