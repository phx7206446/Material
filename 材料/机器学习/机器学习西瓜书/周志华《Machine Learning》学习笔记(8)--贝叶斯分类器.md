上篇主要介绍和讨论了支持向量机。从最初的分类函数，通过最大化分类间隔，max(1/||w||)，min(1/2||w||^2)，凸二次规划，朗格朗日函数，对偶问题，一直到最后的SMO算法求解，都为寻找一个最优解。接着引入核函数将低维空间映射到高维特征空间，解决了非线性可分的情形。最后介绍了软间隔支持向量机，解决了outlier挤歪超平面的问题。本篇将讨论一个经典的统计学习算法--**贝叶斯分类器**。

# **7、贝叶斯分类器**

贝叶斯分类器是一种概率框架下的统计学习分类器，对分类任务而言，假设在相关概率都已知的情况下，贝叶斯分类器考虑如何基于这些概率为样本判定最优的类标。在开始介绍贝叶斯决策论之前，我们首先来回顾下概率论委员会常委--贝叶斯公式。

![1.png](https://i.loli.net/2018/10/18/5bc83fd7a2575.png)

## **7.1 贝叶斯决策论**

贝叶斯决策论（Bayesian decision theory）是概率框架下实施决策的基本方法。对分类任务来说，在所有相关概率都已知的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。下面我们以多分类任务为例来解释其基本原理。

假设有N种可能的类别标记，即$\mathcal{Y}=\{c_1,...,c_N\}$,$\lambda_{ij}$是将一个真实标记为$c_j$的样本误分类为$c_i$所产生的损失。基于后验概率$p(c_i|x)$可获得将样本$x$分类为$c_i$所产生的期望损失（expected loss），即在样本$x$上的“条件风险”（conditional risk）
$$R(c_i|x)=\sum_{j=1}^N\lambda_{ij}P(c_j|x)\tag{7.1}$$
>决策论中将“期望损失”称为“风险”（risk）。

我们的任务是寻找一个判定准则$h$:$\mathcal{X}->\mathcal{Y}$以最小化总体风险：
$$R(h)=E_x[R(h(x)|x)]\tag{7.2}$$
显然，对每个样本$x$，若h能最小化条件风险$R(h(x)|x)$，则总体风险$R(h)$也将被最小化。这就产生了贝叶斯判定准则（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个能使条件风险$R(c|x)$最小的类别标记，即：
$$h^*(x)=argmin_{c\in y}R(c|x)\tag{7.3}$$
即对每个样本$x$,选择能使后验概率$P(c|x)$最大的类别标记。此时，$h^*$称为贝叶斯最优分类器（Bayes optimaml classifier），与之对应的总体风险$R(h^*)$称为贝叶斯风险（Bayes risk）。$1-R(h^*)$反映了分类器所能达到的最好性能，即通过机器学习所能产生的模型精度的理论上限。

若将上述定义中样本空间的划分Bi看做为类标，A看做为一个新的样本，则很容易将条件概率理解为样本A是类别Bi的概率。在机器学习训练模型的过程中，往往我们都试图去优化一个风险函数，因此在概率框架下我们也可以为贝叶斯定义“**条件风险**”（conditional risk）。

![2.png](https://i.loli.net/2018/10/18/5bc83fd15db94.png)

我们的任务就是寻找一个判定准则最小化所有样本的条件风险总和，因此就有了**贝叶斯判定准则**（Bayes decision rule）:为最小化总体风险，只需在每个样本上选择那个使得条件风险最小的类标。

![3.png](https://i.loli.net/2018/10/18/5bc83fd308600.png)

若损失函数λ取0-1损失，则有：

![4.png](https://i.loli.net/2018/10/18/5bc83fd37c502.png)

即对于每个样本x，选择其后验概率P（c | x）最大所对应的类标，能使得总体风险函数最小，从而将原问题转化为估计后验概率P（c | x）。

不难看出，欲使用贝叶斯判定准则来最小化决策风险，首先要获得后验概率$p(c|x)$。然而，在现实任务中这通常难以直接获得，从这个角度来看，机器学习所要实现的是基于有限的训练样本集尽可能准确地估计出后验概率$P(c|x)$。大体来说，主要有两种策略：

	* 判别式模型（discriminative models）：直接对 P（c | x）进行建模求解。例我们前面所介绍的决策树、神经网络、SVM都是属于判别式模型。
	* 生成式模型（generative models）：通过先对联合分布P（x,c）建模，从而进一步求解 P（c | x）。

贝叶斯分类器就属于生成式模型，基于贝叶斯公式对后验概率P（c | x） 进行一项神奇的变换，巴拉拉能量.... P（c | x）变身：

![[Pasted image 20221025145055.png]]

![5.png](https://i.loli.net/2018/10/18/5bc83fd501ad3.png)

对于给定的样本x，P（x）与类标无关被用来归一化的证据因子，P（c）称为类先验概率，p（x | c ）称为类条件概率或称似然。这时估计后验概率P（c | x）就变成为估计类先验概率和类条件概率的问题。对于先验概率和后验概率，在看这章之前也是模糊了我好久，这里普及一下它们的基本概念。

	* 先验概率： 根据以往经验和分析得到的概率。
	* 后验概率：后验概率是基于新的信息，修正原来的先验概率后所获得的更接近实际情况的概率估计。


实际上先验概率就是在没有任何结果出来的情况下估计的概率，而后验概率则是在有一定依据后的重新估计，直观意义上后验概率就是条件概率。下面直接上Wiki上的一个例子，简单粗暴快速完事...

![6.png](https://i.loli.net/2018/10/18/5bc83fd799610.png)

回归正题，对于类先验概率P（c），p（c）就是样本空间中各类样本所占的比例，根据大数定理（当样本足够多时，频率趋于稳定等于其概率），这样当训练样本充足时，p(c)可以使用各类出现的频率来代替。因此只剩下类条件概率p（x | c ），它表达的意思是在类别c中出现x的概率，它涉及到属性的联合概率问题，若只有一个离散属性还好，当属性多时采用频率估计起来就十分困难，例如，假设样本的$d$个属性都是二值的，则样本空间将有2^d种可能的取值。在现实应用中，这个值往往远大于训练样本数$m$，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计$p(x|c)$显然不可行，因为“未被观测到”与“出现概率为零”通常是不同的，因此这里一般采用极大似然法进行估计。

## **7.2 极大似然法**

估计类条件概率的一种常用策略是先假定其具有某种确定的概率分布形式，再基于训练样本对概率分布的参数进行估计。具体地，记关于类别$c$的类条件概率为$P(x|c)$，假设$P(x|c)$具有确定的形式并且被参数向量$\theta_c$唯一确定，则我们的任务就是利用训练集$D$估计参数$\theta_c$。未明确起见，我们将$P(x|c)$记为$P(x|\theta_c)$。

事实上，概率模型的训练过程就是参数估计（parameter estimation）过程。对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派（Frequentist）认为参数虽然未知，但却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值；贝叶斯学派（Bayesian）则认为参数是未观察到的随机变量，其本身也可有分布，因此可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。本节介绍源自频率主义学派的极大似然估计（Maximum Likelihood Estimation，简称MLE），这是根据数据采样来估计概率分布参数的经典方法。

令$D_c$表示训练集$D$中第$c$类样本组成的集合，假设这些样本是独立同分布的，则参数$\theta_c$对于数据集$D_c$的似然是：

![[Pasted image 20221025154021.png]]

对$\theta_c$进行极大似然估计，就是去寻找能最大化似然$P(D_c|\theta_c)$的参数值$\hat{\theta_c}$，直观上看，极大似然估计是试图在$\theta_c$所有可能的取值中，找到一个能使数据出现的“可能性”最大的值。

式（7.9）中的连乘操作易造成下溢，通常使用对数似然（log-likelihood）
$$LL(\theta_c)=logP(D_c|\theta_c)=\sum_{x \in D_c}logP(x|\theta_c)\tag{7.10}$$
此时参数$\theta_c$的极大似然估计$\hat{\theta_c}$为：
$$\hat{\theta_c}=argmax_{\theta_c}LL(\theta_c)\tag{7.11}$$
![[Pasted image 20221025161354.png]]

需要注意的是，这种参数化的方法虽能使类条件概率估计变得相对简单，当估计结果的准确性严重依赖于所假设的概率分布形式是否符合潜在的真实数据分布。在现实应用中，欲做出能较好地接近潜在真实分布的假设，往往需要在一定程度上利用关于应用任务本身的经验知识，否则若仅凭“猜测”来假设概率分布形式，很可能产生误导性的结果。

所以，贝叶斯分类器的训练过程就是参数估计。总结最大似然法估计参数的过程，一般分为以下四个步骤：

	* 1.写出似然函数；
	* 2.对似然函数取对数，并整理；
	* 3.求导数，令偏导数为0，得到似然方程组；
	* 4.解似然方程组，得到所有参数即为所求。

## **7.3 朴素贝叶斯分类器**

不难看出：基于贝叶斯公式来估计后验概率$P(c|x)$的主要困难在于：类条件概率$P(x|c)$使所有属性上的联合概率，难以从有限的训练样本直接估计而得，为避开这个障碍，朴素贝叶斯分类器（naive Bayes classifier）采用了“属性条件独立性假设”，即样本数据的所有属性之间相互独立。这样类条件概率p（x | c ）可以改写为：

![9.png](https://i.loli.net/2018/10/18/5bc83fd55e102.png)

由于对所有类别来说$P(x)$相同，因此基于式(7.6)的贝叶斯判定准则有：

![[Pasted image 20221025162555.png]]

这就是朴素贝叶斯分类器的表达式。

显然，朴素贝叶斯分类器的训练过程就是基于训练集$D$来估计类先验概率$P(c)$，并为每个属性估计条件概率$P(x_i|c)$。

这样，为每个样本估计类条件概率变成为每个样本的每个属性估计类条件概率。

![10.png](https://i.loli.net/2018/10/18/5bc83fd6678cd.png)

相比原始贝叶斯分类器，朴素贝叶斯分类器基于单个的属性计算类条件概率更加容易操作，需要注意的是：若某个属性值在训练集中和某个类别没有一起出现过，这样会抹掉其它的属性信息，因为该样本的类条件概率被计算为0。因此在估计概率值时，常常用进行平滑（smoothing）处理，拉普拉斯修正（Laplacian correction）就是其中的一种经典方法，具体计算方法如下：

![11.png](https://i.loli.net/2018/10/18/5bc83fe54aaed.png)

显然拉普拉斯修正避免了因训练集样本不充分而导致概率估值为零的问题。当训练集越大时，拉普拉斯修正引入的影响越来越小。对于贝叶斯分类器，模型的训练就是参数估计，因此可以事先将所有的概率储存好，当有新样本需要判定时，直接查表计算即可。

在现实任务中朴素贝叶斯分类器有多种使用方法。例如，若任务对预测速度要求较高，则给定训练集，可将朴素贝叶斯分类器所涉及的所有概率估值实现计算好存储起来，这样在进行预测时只需“查表”即可进行判别；若任务数据更替频繁，则可采用“懒惰学习”（lazy learning）方式，先不进行任何训练，待收到预测请求时再根据当前数据集进行概率估值；若数值不断增加，则可在现有估值基础上，仅对新增样本的属性所涉及的概率估值进行计数修正即可实现增量学习。

针对朴素贝叶斯，人们觉得它too sample，sometimes too naive！因此又提出了半朴素的贝叶斯分类器，具体有SPODE、TAN、贝叶斯网络等来刻画属性之间的依赖关系。

## 7.4 半朴素贝叶斯分类器

为了降低贝叶斯公式（7.8）中估计后验概率$P(c|x)$的困难，朴素贝叶斯分类器采用了属性条件独立性假设，但在现实任务中这个假设往往很难成立。于是，人们尝试对属性条件独立性假设进行一定程度的放松，由此产生了一类称为“半朴素贝叶斯分类器”（semi-naive Bayes classifier）的学习方法。

半朴素贝叶斯分类器的基本想法是适当考虑一部分属性间的互相依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽视了比较强的属性依赖关系。“独依赖估计”（one-Dependent Estimator，ODE）是半监督贝叶斯分类器最常用的一种策略。顾名思义，所谓“独依赖”就是假设每个属性在类别之外最多仅依赖于一个其他属性，即：

![[Pasted image 20221025193238.png]]

其中$pa_i$为属性$x_i$所依赖的属性，称为$x_i$的父属性。此时对每个属性$x_i$，若其父属性$pa_i$已知，则可采用类似式（7.20）的方法来估计概率值$P(x_i|c,pa_i)$。于是，问题的关键就转化为了如何确定每个属性的父属性，不同的做法产生不同的独依赖分类器。

最直接的做法是假设所有属性都依赖于同一个属性，称为“超父”（super-parent），然后通过交叉验证等模型选择方法来确定超父属性，由此形成了SPODE（Super-Parent ODE）方法，例如，在图7.1(b)中，$x_1$是超父属性。

![[Pasted image 20221025194756.png]]

TAN(Tree Augmented naive Bayes)则是在最大带权生成树（maximum weighted spanning tree）算法的基础上，通过以下步骤将属性间依赖关系约简薇如图7.1(c)所示的树形结构：

![[Pasted image 20221025195720.png]]

容易看出，条件互信息$I(x_i,x_j|y)$刻画了属性$x_i$和$x_j$在已知类别情况下的相关性，因此，通过最大生成树算法，TAN实际上仅保留了强相关属性之间的依赖性。

AODE（Averaged One-Dependent Estimator）是一种基于集成学习机制、更为强大的独依赖分类器。与SPODE通过模型选择确定超父属性不同，ADOE尝试将每个属性作为超父来构建SPODE，然后将那些具有足够训练数据支撑的SPODE集成起来作为最终结果，即

![[Pasted image 20221025200917.png]]

![[Pasted image 20221025201234.png]]

不难看出，与朴素贝叶斯分类器类似，AODE的训练过程也是“计数”，即在训练数据集上对符合条件的样本进行计数的过程。与朴素贝叶斯分类器相似，AODE无需模型新选择，技能通过预计算节省预测时间，也能采取懒惰学习方式在预测时再进行计数，并且易于实现增量学习。

既然将属性条件独立性假设放松为独依赖假设可能获得泛化性能的提升，那么，能否通过考虑属性间的高阶依赖来进一步提升泛化性能呢？也就是说将$pa_i$替换为包含k个属性的集合$pa_i$，从而将ODE扩展为kDE，需要注意的是，随着k的增加，准确估计概率$P(x_i|y,pa_i)$所需的训练样本数量将以指数级增加，因此，若训练数据非常充分，泛化性能有可能提升，当在有限样本条件下，则又陷入估计高阶联合概率的泥沼。


## 7.5 贝叶斯网

贝叶斯网（Bayesian network）亦称“信念网”（belief network），它借助有向无环图（Directed Acyclic Graph，DAG）来刻画属性之间的依赖关系，并使用条件概率表（Conditional Probability Table,CPT）来描述属性的联合概率分布。

具体来说，一个贝叶斯网$B$由结构$G$和参数$\Theta$两部分构成，即$B=(G,\Theta)$。网络结构$G$是一个有向无环图，其每一个结点对应于一个属性，若两个属性有直接依赖关系，则它们由一条边连接起来；参数$\Theta$定量描述这种依赖关系，假设属性$x_i$在$G$中的父节点集为$\pi_i$，则$\Theta$包含了每个属性的条件概率表$\theta_{x_i|\pi_i}=P_B(x_i|\pi_i)$。

作为一个例子，图7.2给出了西瓜问题的一种贝叶斯网络结构和属性“根蒂”的条件概率表。从图中网络结构可看出，“色泽”直接依赖于“好瓜”和“甜度”，而“根蒂”则直接依赖于“甜度”；进一步从条件概率表能得到“根蒂”对“甜度”量化依赖关系，如$P(根蒂=硬挺|甜度=高)=0.1$等。

![[Pasted image 20221026095431.png]]

### 7.5.1 结构

贝叶斯网结构有效地表达了属性间的条件独立性。给定父节点集，贝叶斯网假设每个属性与它的非后裔属性的独立，于是$B=<G,\Theta>$将属性$x_1,x_2,...,x_d$的联合概率分布定义为：

![[Pasted image 20221026095708.png]]

以图7.2为例，联合概率分布定义为：

![[Pasted image 20221026100102.png]]

显然，$x_3$和$x_4$在给定$x_1$的取值时独立，$x_4$和$x_5$在给定$x_2$的取值时独立，分别简记为![[Pasted image 20221026100453.png]]

图7.3 显示出贝叶斯网中三个变量之间的典型依赖关系，其中两种在式（7.26）中已有所体现。

![[Pasted image 20221026100557.png]]

在“同父”（common parent）结构中，给定父节点$x_1$的取值，则$x_3$与$x_4$条件独立，在“顺序“结构中，给定$x$的值，则$y$与$z$条件独立。$V$形结构（V-structure）亦称”冲撞“结构，给定子结点$x_4$的取值，$x_1$与$x_2$必不独立；奇妙的是，若$x_4$的取值完全未知，则$V$型结构下$x_1$与$x_2$却是相互独立的。简单验证如下：

![[Pasted image 20221026102141.png]]

这样的独立性称为“边际独立性”（marginal independence），记为![[Pasted image 20221026152253.png]]

> 对变量做积分或求和亦称“边际化”（marginalization）

![[Pasted image 20221026153744.png]]

由上述方式产生的无向图称为“道德图”（moral graph），令父结点相连的过程称为“道德化”（moralization）。

>"道德化"的蕴义：孩子的父母应建立牢靠的关系，否则是不道德的。

基于道德图能直观、迅速地找到变量间的条件独立性，假定道德图中有变量$x,y$和变量集合$z=\{z_i\}$,若变量$x$和$y$能在图上被$z$分开，即从道德图中将变量集合$z$去除后，$x$和$y$分属两个连通分支，则称变量$x$和$y$被$z$有向分离，

![[Pasted image 20221026160104.png]]

### 7.5.2 学习

若网络结构已知，即属性间的依赖关系已知，则贝叶斯网的学习过程相对简单，只需通过对训练样本“计数”，估计出每个结点的条件概率表即可。但在显示应用中我们往往不知道网络结构，于是，贝叶斯网学习的首要任务就是根据训练数据集来找出结构最“恰当”的贝叶斯网。“评分搜索”是求解这一问题的常用方法。具体来说，我们先定义一个评分函数（score function），以此来评估贝叶斯网与训练数据的契合程度，然后基于这个评分函数来寻找结构最优的贝叶斯网。显然，评分函数引入了关于我们希望获得什么样的贝叶斯网的归纳偏好。

常用评分函数通常基于信息论准则，此类准则将学习问题看做一个数据压缩任务，学习的目标是找到一个能以最短编码长度描述训练数据的模型，此时编码的长度包括了描述模型自身所需的字节长度和使用该模型描述数据所需的字节长度。对贝叶斯网学习而言，模型就是一个贝叶斯网，同时，每个贝叶斯网描述了一个在训练数据上的概率分布，自有一套编码机制能使那些经常出现的样本有更短的编码，于是我们应该选择那个综合编码长度（包括描述网络和编码数据）最短的贝叶斯网，这就是“最小描述长度”（Minimal Description Length,MDL）准则。

给定训练集$D=\{x_1,x_2,...,x_m\}$，贝叶斯网$B=<G,\Theta>$在$D$上的评分函数可写为：

![[Pasted image 20221026181533.png]]

其中，$|B|$是贝叶斯网的参数个数；$f(\theta)$表示描述每个参数$\theta$所需的字节数；而

![[Pasted image 20221026181624.png]]
是贝叶斯网$B$的对数似然。显然，式（7.28）的第一项是计算编码贝叶斯网$B$所需的字节数，第二项是计算$B$所对应的概率分布$P_B$需要多少字节来描述$D$。于是，学习任务就转化为一个优化任务，即寻找一个贝叶斯网$B$使评分函数$s(B|D)$最小。

![[Pasted image 20221026182554.png]]

![[Pasted image 20221026182922.png]]

### 7.5.3 推断

![[Pasted image 20221026183139.png]]

![[Pasted image 20221026183150.png]]


## 7.6 EM算法

在前面的讨论中，我们一直假设训练样本素有属性变量的值都已被观测到，即训练样本是“完整“的，但在现实应用中往往会遇到”不完整“的训练样本，例如由于西瓜的根蒂已脱落，无法看出是”蜷缩“还是”硬挺“，则训练样本的根蒂属性变量值未知，在这种存在”未观测变量“的情形下，是否仍能对模型参数进行估计呢？

未观测变量的学名是“隐变量”（latent variable）。令$X$表示已观测变量集，$Z$表示隐变量集，$\Theta$表示模型参数，若欲对$\Theta$做极大似然估计，则应最大化对数似然：

![[Pasted image 20221027091251.png]]

然而由于$Z$是隐变量，上式无法直接求解，此时我们可通过对$Z$计算期望，来最大化已观测数据的对数“边际似然”（marginal likelihood）

![[Pasted image 20221027094504.png]]

EM（Expectation-Maximization）算法是常用的估计参数隐变量的利器。它是一种迭代式的方法，其基本思想是：若参数$\Theta$已知，则可根据训练数据推断出最优隐变量$Z$的值（E步）；反之，若$Z$的值已知，则可方便地对参数$\Theta$做极大似然估计（M步）

![[Pasted image 20221027100046.png]]

进一步，若我们不是取$Z$的期望，而是基于$\Theta^t$计算隐变量$Z$的概率分布$P(Z|X,\Theta^t)$,则EM算法的两个步骤是：

![[Pasted image 20221027100841.png]]

![[Pasted image 20221027100949.png]]





