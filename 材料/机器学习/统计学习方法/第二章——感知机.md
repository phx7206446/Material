## 2.1 模型介绍与学习策略

### 2.1.1 模型介绍
感知机是一个二分类的线性分类模型。

- 输入空间：$\mathcal{X} \subseteq R^n$
- 输入：$x=(x^{(1)},x^{(2)},...,x^{(n)})^T \in \mathcal{X}$
- 输出空间：$\mathcal{Y}=\{+1,-1\}$
- 输出：$y\in \mathcal{Y}$
- 感知机：
$$ f(X)=sign(w\cdot x+b)=\begin{cases} +1, & w\cdot x+b\ge 0 \\ -1, & w\cdot x+b \lt 0 \end{cases}$$
其中，$w=(w^{(1)},w^{(2)},...,w^{(n)})^T \in R^n$ 称为权值（Weight），$b\in R$称为偏置（Bias）,$w\cdot x$表示内积：
$$w\cdot x=w^{(1)}x^{(1)}+w^{(2)}x^{(2)}+...+w^{(n)}x^{(n)}$$
- 假设空间：$\mathcal{F}=\{f|f(x)=w\cdot x+b\}$

几何含义：
线性方程：$w\cdot x+b=0$

- 特征空间$R^n$中的一个超平面$S$
- 法向量：$w$；截距：$b$

![[Pasted image 20220511185050.png]]

超平面：比他所处环境小一维的子空间。

感知机流程图：

![[Pasted image 20220511185305.png]]


### 2.1.2 学习策略
学习策略就是优化损失函数（目标函数）

条件：要求数据集必须是线性可分的。

线性可分定义：

![[Pasted image 20220511185409.png]]

- $\forall x_0\in R^n$到$S$的距离：
$$
\frac{1}{||w||}\lvert w\cdot x+b \rvert
$$
 >![[Pasted image 20220511190041.png]] 
- 若$x_0$是正确分类点，则：

$$ \frac{1}{||w||}\lvert w\cdot x_0+b\rvert=\begin{cases}  \frac{w\cdot x_0+b}{||w||}, &y_0=+1 \\ -\frac{w\cdot x_0+b}{||w||}, & y0=-1 \end{cases}$$
- 若$x_0$是错误分类点，则：
$$ \frac{1}{||w||}\lvert w\cdot x_0+b\rvert=\begin{cases}  -\frac{w\cdot x_0+b}{||w||}, &y_0=+1 \\ \frac{w\cdot x_0+b}{||w||}, & y0=-1 \end{cases}=-\frac{y_0(w\cdot x_0+b)}{||w||}$$

- 误分类点$x_i$到$S$的距离：
$$-\frac{1}{||w||}y_i(w\cdot x_i+b)$$

- 所有误分类点到$S$的距离：
$$-\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
其中，$M$代表所有误分类点的集合。
- 损失函数：
$$
L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
因为$\frac{1}{||w||}$不影响上述式子的正负，且算法的终止条件应该是不存在误分类的点，所以应该为0，故此时$\frac{1}{||w||}$可以忽略。


## 2.2 准备知识——梯度下降法
### 2.2.1 直观理解
![[Pasted image 20220511191627.png]]

### 2.2.2 概念
- 梯度：指某一函数在该点处最大的方向导数，沿着该方向可取得最大的变化率。
$$\nabla=\frac{\partial f(\theta)}{\partial \theta}$$
- 若$f(\theta)$是凸函数，可通过梯度下降法进行优化。

$$\theta^{k+1}=\theta^{k}-\eta\nabla f(\theta^k)$$
梯度下降法算法：

![[Pasted image 20220511192206.png]]

例子：
![[Pasted image 20220511192307.png]]

### 2.2.3 原理
$f(\theta)$可微，所以可以使用泰勒公式近似。

![[Pasted image 20220511192420.png]]

![[Pasted image 20220511192546.png]]

![[Pasted image 20220511192628.png]]

 
## 2.3 学习算法之原始形式
### 2.3.1 学习问题
- 训练数据集：
$$T=\{(x_1.y_1),(x_2,y_2),...,(x_N,y_N)\}$$
其中，$x_i\in \mathcal{X} \subseteq R^n$，$y_i\in \mathcal{Y}=\{+1,-1\}$。

- 损失函数：
$$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
其中，$M$代表所有误分类点的集合。

- 模型参数估计：
$$argmin_{w,b}L(w,b)$$

### 2.3.2 原始形式

对于感知机模型，采用随机梯度下降算法。

损失函数：
$$L(w,b)=-\sum_{x_i\in M}y_i(w\cdot x_i+b)$$
梯度：
$$\nabla_wL(w,b)=-\sum_{x_i\in M}y_ix_i;\nabla_bL(w,b)=\sum_{x_i\in M}y_i$$
参数更新：
- 批量梯度下降算法（Batch Gradient Descent）：每次迭代时，使用所有误分类点来进行参数更新。

$$w\leftarrow w+\eta\sum_{x_i\in M}y_ix_i;b\leftarrow b+\eta\sum_{x_i\in M}y_i$$
其中，$\eta(0\le \eta \le1)$代表步长。
- 随机梯度下降法（Stochastic Gradient Descent）：每次随机选取一个误分类点。
$$w\leftarrow w+\eta y_ix_i;b\leftarrow b+\eta y_i$$
原始形式算法：

![[Pasted image 20220511194237.png]]

### 2.3.3 例题分析
输入：训练接：
$$T=\{(x_1,+1),(x_2,+1),(x_3,-1)\}$$
其中，$x_1=(3,3)^T,x_2=(4,3)^T,x_3=(1,1)^T$，假设$\eta=1$。

输出：$w.b$；感知机模型$f(x)=sign(w\cdot x+b)$

![[Pasted image 20220511194712.png]]

学习问题：
$$argmin_{w,b}L(w,b)=argmin_{w,b}[-\sum_{x_i\in M}y_i(w\cdot x_i+b)]$$
![[Pasted image 20220511194841.png]]

![[Pasted image 20220511194951.png]]

![[Pasted image 20220511195012.png]]

![[Pasted image 20220511195035.png]]


## 2.4 学习算法之对偶形式

### 2.4.1 对偶形式
- 在原始形式中，若$(x_i,y_i)$为误分类点，可如下更新参数：
$$w\leftarrow w+\eta y_ix_i;b\leftarrow b+\eta y_i$$
- 假设初始值$w_0=0$，$b_0=0$，对误分类点$(x_i,y_i)$通过上述公式更新参数，修改$n_i$次之后，$w,b$的增量分别为$\alpha_i y_ix_i$和$\alpha_i y_i$，其中$\alpha_i=n_i\eta$。

- 最后学习到的参数为：
$$w=\sum_{i=1}^N\alpha_iy_ix_i;b=\sum_{i=1}^N\alpha_iy_i$$
![[Pasted image 20220511195749.png]]

可以发现对偶形式的算法主要考虑将每个样本点的梯度作为一个常量，求解的是对某个样本点的更新次数。

对偶形式：算法
即将参数w用上述的发现替代。
![[Pasted image 20220511200155.png]]

![[Pasted image 20220511201807.png]] 

在上图中，我们发现迭代条件中有N个内积，为了简化计算复杂度，我们可以提前计算好，存在Gram矩阵中。

### 2.4.2 例题分析
![[Pasted image 20220511202025.png]]

![[Pasted image 20220511202105.png]]

![[Pasted image 20220511202434.png]]

![[Pasted image 20220511202557.png]]

![[Pasted image 20220511202835.png]]

## 2.5 算法收敛性证明——原始形式算法

记$\hat{w}=(w^T,b^T)$，$\hat{x}=(x^T,1)^T$，则分离超平面可以写为：
$$\hat{w}\cdot \hat{x}=0$$
定理：

![[Pasted image 20220511203404.png]]

具体证明：

![[Pasted image 20220511203552.png]]

![[Pasted image 20220511210800.png]]

![[Pasted image 20220511211805.png]]

![[Pasted image 20220511212306.png]]

![[Pasted image 20220511212357.png]]

上述定理总结：

- 收敛性：对于线性可分的$T$，经过有限次搜索，可得将$T$完全正确分开的分离超平面。
- 对于线性不可分的$T$,算法不收敛，迭代结果会发生震荡。
- 依赖性：不同的初值选择，或者迭代过程中不同的误分类点选择顺序，可能会得到不同的分离超平面。
- 为得到唯一分类超平面，需增加约束条件。
