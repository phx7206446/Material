## 1.1 统计学习的定义与分类
### 1.1.1 统计学习的概念
![[Pasted image 20220507204441.png]]


统计学习（Statistical Mathine Learning) 是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。

主要特点：
1.以计算机和网络为平台
2.以数据为研究对象
3.以预测和分类数据为目的
4.以方法为中心
5.是多领域交叉的学科

统计学习方法的步骤：
1.得到一个有限的训练数据集合
2.确定学习模型的集合——模型
3.确定模型选择的准则——策略
4.实现求解最优模型的算法——算法
5.通过学习方法选择最优模型
6.利用学习的最优模型对新数据进行预测或分析

![[Pasted image 20220507205257.png]]


### 1.1.2 统计学习的分类

![[Pasted image 20220507205358.png]]


## 1.2 统计学习方法的基本分类

### 1.2.1 监督学习
#### 1.2.1.1 定义

![[Pasted image 20220510144545.png]]

监督学习(Supervised Learning) 是指从标注数据中学习与预测模型的机器学习问题，其本质是学习输入到输出的映射的统计规律。

映射：两个集合中的元素相互对应的关系。

![[Pasted image 20220510144810.png]]

#### 1.2.1.2 相关概念

输入空间（Input Space)：输入的所有可能取值的集合。

实例（Instance)：每一个具体的输入，通常由特征向量（Feature Vector）表示

特征空间（Feature Space）：所有特征向量存在的空间

在一般情况下输出空间与特征空间相同。

![[Pasted image 20220510145209.png]]

上述$\phi(x)$将输入空间映射到特征空间。

根据变量的不同类型可分为:
- 输入变量与输出变量均为连续变量的预测问题——回归问题
- 输出变量为有限个离散变量的预测问题——分类问题
- 输入变量与输出变量均为变量序列的预测问题——标注问题

监督学习的基本假设：$X$和$Y$具有联合概率分布$P(X,Y)$

监督学习的目的：学习一个输入到输出的映射，这一映射以模型表示

模型的形式：条件概率分布$P(Y|X)$或决策函数$Y=f(X)$

假设空间（Hypothesis Space）：所有这些可能模型的集合


监督学习流程图：

![[Pasted image 20220510145954.png]]



### 1.2.2 无监督学习

#### 1.2.2.1 定义
无监督学习（Unsupervised Learning）是指从无标注数据中学习预测模型的机器学习问题，其本质是学习数据中的统计规律或潜在结构。

![[Pasted image 20220510150250.png]]


#### 1.2.2.2 流程图
![[Pasted image 20220510150551.png]]


### 1.2.3 强化学习

#### 1.2.3.1 流程图
![[Pasted image 20220510150657.png]]

![[Pasted image 20220511161154.png]]

•状态转移概率函数：$P(s'|s,a)$

•奖励函数：$r(s,a)$

•策略π：给定状态下动作的函数$a=f(x)$或者条件概率分布$P(a|s)$

•状态价值函数:$V_{\pi}=E_{\pi}[r_{t+1}+\gamma r_{t+2}+.....|s_t=s]$

•动作价值函数：$q_{\pi}(s,a)=E_{\pi}[r_{t+1}+\gamma r_{t+2}+...|s_t=s,a_t=a]$

强化学习方法：
•无模型（model-free）：

    •基于策略（policy-based）：求解最优策略π*

    •基于价值（value-based）：求解最优价值函数

•有模型（model-based）

    •通过学习马尔可夫决策过程的模型，包括转移概率函数和奖励函数

    •通过模型对环境的反馈进行预测

    •求解价值函数最大的策略π*


### 1.2.4 半监督学习(SSL)

•少量标注数据，大量未标注数据

•利用未标注数据的信息，辅助标注数据，进行监督学习

•较低成本

### 1.2.5 主动学习(AL)

•机器主动给出实例，教师进行标注

•利用标注数据学习预测模型


### 1.2.5 其他分类方式
•按技巧分类：

贝叶斯学习（Bayesian learning）：
![[Pasted image 20220511162027.png]]      

![[Pasted image 20220511162035.png]]

核方法（Kernel Method）：
•使用核函数表示和学习非线性模型，将线性模型学习方法扩展到非线性模型的学习

•不显式地定义输入空间到特征空间的映射，而是直接定义核函数，即映射之后在特征空间的内积

•假设x1，x2是输入空间的任意两个实例，内积为<x1, x2>，输入空间到特征空间的映射为φ，

核方法在输入空间中定义核函数 K(x1, x2)，使其满足 K(x1, x2) = < φ(x1), φ(x2)>



## 1.3 统计学习方法的三要素

![[Pasted image 20220510150757.png]]

### 1.3.1 监督学习

#### 1.3.1.1 模型

![[Pasted image 20220510151101.png]]

举例：线性回归：

![[Pasted image 20220510151143.png]]

![[Pasted image 20220510151229.png]]

举例：Logistic回归

![[Pasted image 20220510151254.png]]


#### 1.3.1.2 策略

策略：选择最优模型。

- 损失函数：度量模型一次预测的好坏，记作$L(Y,f(X))$
- 风险函数：度量平均意义下模型预测的好坏：
$$
\begin{aligned}
R_{exp}(f)&=E_p[L(Y,f(X))]\\
&=\int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)dxdy
\end{aligned}$$
但在实际情况下，联合分布$P(x,y)$并不是已知的，此时我们考虑使用经验风险。

- 经验风险：模型$f(X)$关于训练集的平均损失。
$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$
其中，训练集$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$


四种常见的损失函数：
- 0-1损失函数（0-1 Loss Function）
$$ L(Y,f(X))=\begin{cases} 1, & Y\ne f(X) \\ 0, & Y=f(X) \end{cases}$$

- 平方损失函数（Quadratic Loss Function）
$$L(Y,f(X))=(Y-f(X))^2$$
- 绝对损失函数（Absolute Loss Function)
$$
L(Y,f(X))=\lvert Y-f(X)\rvert$$
- 对数损失函数（Logarithmic Loss Function）
$$L(Y,P(Y|X))=-logP(Y|X)$$

在这前，我们学习了风险函数和经验风险函数，当训练集样本$N->\infty$时，根据大数定理，此时经验风险函数就近似于风险函数，即：

$$
R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))->R_{exp}(f)=E_p[L(Y,f(X))],N->\infty
$$

所以在一定程度上，使用经验风险损失作为风险函数的估计值是合理的。

在现实情况下，样本容量$N$是有限的，有时甚至很小，所以仅仅用经验风险来估计风险函数效果并不理想，需要对其进行一定的校正。


当样本容量$N$较大的时候，我们可以使用经验风险作为风险函数的估计值，即进行经验风险最小化：
$$
min_{f\in \mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$
当样本容量$N$较小时，仅仅使经验风险最小化，容易出现过拟合的现象，于是我们引入了结构风险的概念，即在经验风险基础上增加一个针对模型复杂度的惩罚项，平衡了经验风险和模型复杂度。

$$R_{srm}=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$

于是我们可以使用结构风险最小化：

$$min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$
在监督学习中，我们通过优化经验风险或者结构风险来达到模型学习的目的。

#### 1.3.1.3 算法
- 算法：如何求解最优模型的问题
- 若优化问题存在显示解析解，算法简易
- 通常不存在解析解，需要数值计算方法，比如梯度下降法


### 1.3.2 无监督学习
- 模型：函数$z=g_{\theta}(x)$,条件概率分布$P_{\theta}(z|x)$或条件概率分布$P_{\theta}(x|z)$
- 策略：优化目标函数
- 算法：通常是迭代算法


## 1.4 模型的评估与选择

### 1.4.1 训练误差与测试误差

监督学习流程图

![[Pasted image 20220510190648.png]]

#### 1.4.1.1 训练误差
- 学习到的模型：$Y=f(X)$
- 训练集（Training Set）：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$$
- 训练误差（Training Error）:
$$R_{emp}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$
#### 1.4.1.2 测试误差
- 学习到的模型：$Y=f(X)$
- 测试集（Test Set）：
$$T'=\{(x_1',y_1'),(x_2',y_2'),...,(x_n',y_n')\}$$
- 测试误差（Test Eroor)：
$$e_{test}=\frac{1}{N'}\sum_{i'=1}^N'L(y_{i'},f(x_{i'}))$$

#### 1.4.2 误差率与准确率

- 误差率（Eroor Rate）：
$$e_{test}=\frac{1}{N'}\sum_{i'=1}^N'I(y_{i'} \ne f(x_{i'}))$$
- 准确率（Accuracy）：
$$r_{test}=\frac{1}{N'}\sum_{i'=1}^N'I(y_{i'}=f(x_{i'}))$$
注：
$$r_{test}+e_{test}=1$$

### 1.4.2 过拟合与模型选择

#### 1.4.2.1 多项式拟合案例
例：真实函数为$y=\sin(2\pi x)$ ，样本为$y_i=\sin(2\pi x_i)+\epsilon_i$，训练集为$T=\{(x_1,y_1),(x_2,y_2),...,(x_{10},y_{10}\}$。

![[Pasted image 20220510192645.png]]

M次多项式：
$$f_M(x,w)=w_0+w_1x+w_2x^2+。。。+w_Mx^M$$
经验风险：
$$L(w)=\frac{1}{2}\sum_{i=1}^N(f_M(x_i,w)-y_i)^2$$

带入多项式，通过最小二乘法求解参数。

![[Pasted image 20220510195917.png]]


#### 1.4.2.2 过拟合

过拟合（Over-Fitting）：学习所得模型包含参数过多，出现对已知数据预测很好，但对未知数据预测很差的现象。

![[Pasted image 20220510200104.png]]

![[Pasted image 20220510200351.png]]


## 1.5 正则化与交叉验证

### 1.5.1 正则化

正则化：实现结构风险最小化策略。

- 一般形式：
$$min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$
- 经验风险：
$$\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$
- 正则化项：
$$\lambda J(f)$$
其中，$\lambda$权衡经验风险和模型复杂度。


正则化项：
- $L_1$范数
$$L(w)=\frac{1}{N}\sum_{i=1}^N(f(x_i;w)-y_i)^2+\lambda ||w||_1$$
其中，$||w||_1=\sum_j\lvert w_j\rvert$

$L_1$范数会使得某些参数变为0，就有特征筛选的效果，会使参数稀疏化，即洗漱的模型。

- $L_2$范数
$$L(w)=\frac{1}{N}\sum_{i=1}^N(f(x_i;w)-y_i)^2+\frac{\lambda}{2}||w||_2^2$$
其中，$||w||_2=\sqrt{\sum_jw_j^2}$,$||w||_2^2=\sum_jw_j^2$

$L_2$范数会使得某些参数接近0，会使得模型更简单，但没有特征筛选的作用。

奥卡姆剃刀原理：在模型选择时，选择所有可能模型中，能很好解释已知数据并且十分简单的模型。


### 1.5.2 交叉验证

当数据充足的情况下，可以将数据集划分为训练集，验证集和测试集。

- 训练集（Training Set）：用以训练模型
- 验证集（Validation Set）：用以选择模型
- 测试集（Test Set）：用以最终对学习方法的评估


当数据集不足时，可以考虑交叉验证的方法。
- 简单交叉验证：随机将数据分为两部分，即训练集和测试集。（7：3）
- S折交叉验证：随机将数据分为S个互补相交、大小相同的子集，其中以S-1个子集作为训练集，余下的子集作为测试集。

![[Pasted image 20220510210809.png]]

- 留一交叉验证：S折交叉验证的特殊情形，S=N


## 1.6 泛化能力

### 1.6.1 泛化误差
泛化误差：若所学习到的模型是$f$，那么这个模型对未知数据预测的误差即为泛化误差（Generalization Error）。

$$\begin{aligned}
R_{exp}(f)&=E_p[L(Y,f(X))]\\
&=\int_{\mathcal{X}\times\mathcal{Y}}L(y,f(x))P(x,y)dxdy
\end{aligned}
$$

### 1.6.2 泛化误差上界
泛化误差上界（Gneralization Error Bound）:指泛化误差的概率上界。两种学习方法的优劣，通常通过他们的泛化误差上界进行比较。

性质：
- 样本容量的函数：当样本容量增加时，泛化上界趋于0。
- 假设空间容量的函数：假设空间容量越大，模型就越难学，泛化误差上界就越大。


### 1.6.3 泛化误差上界：二分类

考虑如下二分类问题：
- 训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
其中，$T$是从联合概率分布$P(X,Y)$独立同分布产生的，$X\in R^n$,$Y\in \{-1,+1\}$

- 假设空间：
$$\mathcal{F}=\{f_1,f_2,f_3,...,f_d\}$$
其中，$d$是函数个数。


假设$f\in \mathcal{F}$，损失函数为0-1损失，

- 期望风险：
$$R(f)=E[L(Y,f(X))]$$
- 经验风险：
$$\hat{R}(f)=\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))$$
- 风险经验最小化
$$f_N=argmin_{f\in \mathcal{F}}R(f)$$
- $f_N$的泛化能力：
$$R(f_N)=E[L(Y,f_N(X))]$$


二分类问题泛化误差上界定理：

![[Pasted image 20220511094704.png]]

有上述定义可知，当$N->\infty$ 时，后一项趋于0，当$d$越大时，即假设空间越复杂，后一项也会随之增大。

二分类泛化误差上界定理证明：

预备知识：

![[Pasted image 20220511095233.png]]

![[Pasted image 20220511095514.png]]

 ![[Pasted image 20220511095718.png]]

正式证明：
![[Pasted image 20220511100136.png]]

右侧（2）-（3）原因：x,y是从联合分布P(X,Y)中独立同分布产生的。

![[Pasted image 20220511100607.png]]

![[Pasted image 20220511100750.png]]


## 1.7 生成模型与判别模型
![[Pasted image 20220511101309.png]]


### 1.7.1 生成模型和判别模型
#### 1.7.1.1 生成模型
![[Pasted image 20220511102138.png]]

典型的生成模型：朴素贝叶斯法、隐马尔可夫模型

注：输入和输出变量要求为随机变量


#### 1.7.1.2 判别模型

![[Pasted image 20220511103039.png]]

典型的判别模型：K近邻法、感知机、决策树等

注：不需要输入和输出变量均为随机变量

### 1.7.2 生成模型VS判别模型

举例如下：
![[Pasted image 20220511153609.png]]

上图中表示分类任务，即区分小狗与大象，其中比较明显的一个不同点在于鼻子的长短。判别方法就可以通过动物鼻子的长短来判断其的类别。生成模型的方法则需要分别构建小狗和大象的生成模型，这里就不止包含鼻子，耳朵，嘴巴，眼睛，四肢等等，对于新的图像，我们将其分别与小狗与大象的特征模型相比较，与哪一个特征相似较高，则判断为哪一类。

简单来说，生成模型需要从大量数据中寻找规律，通过学习数据是如何生成的，然后再对数据进行分类。而判别模型只是针对差别进行分类，具有针对性。

两者之间的具体差异如下：

生成模型：
- 所需数据量较大
- 可还原联合概率分布$P(X,Y)$
- 收敛速度更快
- 能反映同类数据本身的相似度。
- 隐变量存在时，仍可用生成模型

判别模型：
- 所需样本的数量少于生成模型
- 可直接面对预测，准确率更高
- 可简化学习问题
- 不可以反映数据本身的特性


## 1.8 监督学习应用
### 1.8.1 分类问题
流程图：

![[Pasted image 20220511154632.png]]

分类准确率：

![[Pasted image 20220511154753.png]]

二分类问题：

![[Pasted image 20220511154833.png]]

二分类问题的评价指标：
- 精确率：
$$
P=\frac{TP}{TP+FP}
$$
- 召回率：
$$R=\frac{TP}{TP+FN}$$
- 调和值（F1）：
$$\frac{2}{F_1}=\frac{1}{P}+\frac{1}{R}$$
方法和应用领域：

方法：
- 感知机
- $k$近邻
- 朴素贝叶斯
- 决策树
- Logistic回归

应用：
- 银行业务
- 网络安全
- 图像处理
- 手写识别
- 互联网搜索


### 1.8.2 标注问题
•输入：观测序列， 输出：标记序列或状态序列

•训练集：
$$T=\{(x_1,y_1),(x_2,y_2),...(x_N,y_N)\}$$
• 输入观测序列：
    $$x_i=(x_i^{(1)},x_i^{(2)},x_i^{(3)},...,x_i^{(n)})^T$$

 • 输出标记序列 
   
 $$y_i=(y_i^{(1)},y_i^{(2)},y_i^{(3)},...,y_i^{(n)})^T$$

- 学习的模型：
$$P(Y^{(1)}.Y^{(2)}....,Y^{(n)}|X^{(1)},X^{(2)},...,X^{(n)})$$

- 预测

新的输入观测序列：
$$x_{N+1}=(x_{N+1}^{(1)},x_{N+1}^{(2)},...,x_{N+1}^{(n)})^T$$

预测的输入观测序列：
$$y_{N+1}=(y_{N+1}^{(1)},y_{N+1}^{(2)},...,y_{N+1}^{(n)})^T$$
![[Pasted image 20220511163249.png]]

评价指标：与分类问题类似。

方法：隐马尔可夫模型，条件随机场

应用：信息抽取，自然语言处理

![[Pasted image 20220511163402.png]]


### 1.8.3 回归问题
![[Pasted image 20220511163532.png]]

- 类型：
- 按输入变量个数：一元回归、多元回归
- 按输入和输出变量之间关系：线性回归、非线性回归

- 损失函数：平方损失
- 应用：商务领域
