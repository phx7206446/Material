## 4.1 核心——贝叶斯定理
贝叶斯思维：

![[Pasted image 20220512160846.png]]

### 4.1.1 条件概率
已知：现有一盒巧克力，一共装了16块。其中黑色、白色、棕色巧克力各4块，红色、黄色巧克力各2块
问：
- 随机取出一块黑色巧克力的可能性是多少？ $\frac{4}{16}$
- 随机取出一块红色巧克力的可能性是多少？$\frac{2}{16}$

![[Pasted image 20220512162317.png]]

问：已知巧克力出自A盒，取出黑色巧克力的概率：

解：
$$P(Black|BoxA)=\frac{P(Black and BoxA)}{P(BoxA)}$$
- $P(Black and BoxA)=\frac{3}{16}$
- $P(BoxA)=\frac{7}{16}$

条件概率公式：
$$P(X=x|Y=y)=\frac{P(X=x,Y=y)}{P(Y=y)}$$
### 4.1.2 贝叶斯定理
考虑之前的例子：

问：已知取出一块黑色巧克力，它来自A盒的概率：

解：
$$P(BoxA|Black)=\frac{P(Black and BoxA)}{P(Black)}$$
- $P(Black and BoxA)=P(Black|BoxA)\cdot P(BoxA)$
- $P(Black)=P(Black|BoxA)\cdot P(BoxA)+P(Black|BoxB)\cdot P(BoxB)$

已知：
存在$K$类$c_1,c_2,...,c_k$，给定一个新的实例$x=(x^{(1)},x^{(2)},...,x^{(n)})$

问：该实例归属第$c_i$类的可能性有多大？
$$P(Y=c_i|X=x)=\frac{P(X=x|Y=c_i)\cdot P(Y=c_i)}{P(X=x)}$$
即，
$$P(Y=c_i|X=x)=\frac{P(X=x|Y=c_i)\cdot P(Y=c_i)}{\sum_{i=1}^KP(X=x|Y=c_i)\cdot P(Y=c_i)}$$

贝叶斯分类：

已知：有A和B两盒巧克力，现在拿到一块黑色的巧克力。
问：该巧克力最有可能是哪个盒子的？

- $$P(BoxA|Black)=\frac{P(Black|BoxA)\cdot P(BoxA)}{P(Black)}$$
- $$P(BoxB|Black)=\frac{P(Black|BoxB)\cdot P(BoxB)}{P(Black)}$$
贝叶斯分类：选取概率最大的那一类。


### 4.1.3 朴素贝叶斯

补充：https://blog.csdn.net/weixin_33445134/article/details/112543130?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-9-112543130-blog-102845028.pc_relevant_paycolumn_v3&spm=1001.2101.3001.4242.6&utm_relevant_index=12

之所以使用朴素贝叶斯是因为可行性。

假设：实例特征之间相互独立

![[Pasted image 20220512165339.png]]

可以发现每一类的条件概率分布分母都是相同的，所以只需考虑分子即可。



## 4.2 基本方法
原始贝叶斯：由于样本维度可能较大，且样本数量较小，所以可能无法完全覆盖整个组合空间，使用极大似然估计的方式，去估计条件概率分布，即先假设条件概率分布符合某个分布，再用对数似然的方式去求的对应的参数。

原始贝叶斯的例子可见：https://zh.d2l.ai/chapter_linear-networks/linear-regression.html#subsec-normal-distribution-and-squared-loss
中的3.1.13

朴素贝叶斯法也被称为朴素贝叶斯分类，从根本上 来说，是一个分类方法。

![[Pasted image 20220512165648.png]]

### 4.2.1 “朴素”？

训练数据集：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
- 输入：$\mathcal{X}\subseteq R^n,x\in \mathcal{X}$
- 输出：$\mathcal{Y}=\{c_1,c_2,...,c_k\},y\in \mathcal{Y}$

生成方法：学习联合概率分布$P(X,Y)$。

- 先验概率分布：
$$P(Y=c_i),i=1,2,3,...,K$$
- 条件概率分布：
$$P(X=x|Y=c_i)=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_i)$$
- 联合概率分布：
$$P(X,Y)=P(X=x|Y=c_i)P(Y=c_i),i=1,2,...K$$
例子：两盒巧克力

![[Pasted image 20220512212920.png]]

先验概率分布即A盒和B盒中巧克力的个数，分别是$\frac{7}{16}和\frac{9}{16}$

![[Pasted image 20220512213026.png]]

朴素贝叶斯：即在贝叶斯方法上加上了条件独立性假设，即n个特征相互独立。

为什么需要条件独立假设：

![[Pasted image 20220512213402.png]]

若不考虑条件独立假设，此时的组合数呈指数上升。、

假设：实例特征之间相互独立：
$$
\begin{aligned}
P(X=x|Y=c_j)&=P(X^{(1)}=x^{(1)},...,X^{(n)}=x^{(n)}|Y=c_i)\\
&=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)
\end{aligned}
$$
### 4.2.2 后验概率最大化

朴素贝叶斯分类：

已知：
存在$K$类$c_1,c_2,...,c_k$，给定一个新的实例$x=(x^{(1)},x^{(2)},...,x^{(n)})$

问：该实例归属哪一类？

- 后验概率：
$$P(Y=c_i|X=x)=\frac{P(Y=c_i)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)}{\sum_{i=1}^KP(Y=c_i)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)}$$
- 分类：
$$y=argmax_{c_i}P(Y=c_i)\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_i)$$

- 0-1 损失函数（0-1 Loss Function）
$$ L(Y,f(X))=\begin{cases} 1, & Y\ne f(X) \\ 0, & Y=f(X) \end{cases}$$
- 期望风险
$$R(f)=E[L(Y,f(X))]$$
- 后验概率最大化
$$f(x)=argmax_{c_i}P(c_i|X=x)$$

## 4.3 极大似然估计
### 4.3.1 朴素贝叶斯
分类：
$$y=argmax_{c_k}P(Y=c_k)=\prod_{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)$$
- 先验概率
$$P(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)}{N},k=1,2,3,...,K$$
- 条件概率
$$P(X^{(j)}=a_{jI}|Y=c_k)=\frac{\sum_{i=1}^NI(x_i^{(j)}=a_{jI},y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}$$
### 4.3.2 极大似然法
#### 4.3.2.1 原理
极大似然法本质上就是概率最大化。

原理：使似然函数（即联合密度函数）达到最大的参数值。

- 假设$X$的密度函数为$f(X,\beta)$,如果简单随机样本$X_1,X_2,...,X_N$相互独立，则其联合密度函数为：
$$L(x_1,x_2,...,x_N;\beta)=\prod_{i=1}^Nf(x_i,\beta)$$
- 当$(X_1,X_2,...,X_N)$取定值$(x_1,x_2,...,x_N)$时，$L(x_1,...,x_N;\beta)$是$\beta$的函数，即样本的似然函数。
- $\beta$的极大似然估计$\hat{\beta}$：
$$\hat{\beta}=argmax_{\beta\in \theta}L(x_1,...,x_N;\beta)$$
- 记似然函数$L(\beta)=(x_1,x_2,...,x_N;\beta)$


#### 4.3.2.2 实现
举例：

![[Pasted image 20220513101247.png]]

![[Pasted image 20220513101638.png]]

极大似然估计：数值计算：

![[Pasted image 20220513102044.png]]

例子
![[Pasted image 20220513102206.png]]

![[Pasted image 20220513102507.png]]

极大似然估计：迭代法


朴素贝叶斯：极大似然估计联系

其实，先验概率或者条件概率就是极大似然估计。

补充链接 https://blog.csdn.net/weixin_33445134/article/details/112543130?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-9-112543130-blog-102845028.pc_relevant_paycolumn_v3&spm=1001.2101.3001.4242.6&utm_relevant_index=12

## 4.4 朴素贝叶斯法——算法
### 4.4.1 算法详解
输入：训练集：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
实例$x=(x^{(1)},x^{(2)},...,x^{(n)})$

输出：实例$x$所属类别$y$。

![[Pasted image 20220513105521.png]]


### 4.4.2 例题解释

![[Pasted image 20220513110103.png]]

![[Pasted image 20220513111158.png]]



## 4.5 贝叶斯估计
### 4.5.1 估计方法
- 先验概率的贝叶斯估计：
$$P_{\lambda}(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}$$
- 条件概率的贝叶斯估计：
- $$P_{\lambda}(X^{(j)}=a_{jI}|Y=c_k)=\frac{\sum_{i=1}^NI(X_i^{(j)}=a_{jI}|y_i=c_k)+\lambda}{\sum_{i=1}^NI(y_i=c_k)+S_j\lambda}$$
>注：$\lambda \ge 0$,当$\lambda=0$时为极大似然估计，$\lambda=1$时为拉普拉斯平滑（Laplacian Smoothing）

- 为什么称作贝叶斯估计？

![[Pasted image 20220513141046.png]]

![[Pasted image 20220513141311.png]]

最大化后验估计：

![[Pasted image 20220513152159.png]]

![[Pasted image 20220513153105.png]]

例2：

![[Pasted image 20220513152627.png]]

![[Pasted image 20220513152905.png]]

可知均匀分布为无信息先验。

- 平滑思想是什么？
- 贝叶斯估计：
$$P_\lambda(Y=c_k)=\frac{\sum_{i=1}^NI(y_i=c_k)+\lambda}{N+K\lambda}$$
- 正则化：
$$min_{f\in F}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$
![[Pasted image 20220513153650.png]]

可以发现贝叶斯估计就是极大似然估计和先验概率的凸组合，如果对照之前的正则化，那么我们可以认为先验就是我们加入的正则化项。

### 4.5.2 例题解说

![[Pasted image 20220513154039.png]]







