## 5.1 一颗有理想的树
### 5.1.1 思维模式
归纳法：

![[Pasted image 20220513154242.png]]

![[Pasted image 20220513213731.png]]

### 5.1.2 模型结构
决策树：分类决策树模型是一种描述对实例进行分类的树形结构。

![[Pasted image 20220513213907.png]]

if-then 规则：决策树是通过一系列规则对数据进行分类的过程。

构件决策树：

![[Pasted image 20220513214225.png]]

如何选择最优特征？

## 5.2 条件概率分布与决策树学习
### 5.2.1 条件概率分布
- 决策树：给定特征条件下类的$P(Y|X)$。
- 条件概率分布：特征空间的一个划分（Partition）；
- 划分：单元（Cell）或区域（Region）互不相交。

![[Pasted image 20220516191320.png]]

当某单元$c$的条件概率满足$P(Y=+1|X=c)\gt 0.5$时，则认为该单元属于正类。

![[Pasted image 20220516195733.png]]

- 一条路径对应于划分中的一个单元；
- 决策树的条件概率分布由各个单元给定条件下类的条件概率分布组成

![[Pasted image 20220516195944.png]]


### 5.2.2 决策树学习

已知：训练集：
$$T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}$$
其中$x_i=(x_i^{(1)},x_i^{(2)},...,x_i^{(n)})^T,y_\{1,2,...,K\},i=1,2,...,N;$

目的：构造决策树，并对实例正确分类。


![[Pasted image 20220516201551.png]]

- 本质：从训练数据集中归纳出一组分类规则，与训练数据集不相矛盾。
- 假设空间：由无穷多个条件概率模型组成。
- 一颗好的决策树：与训练数据矛盾较少，同时具有很好的泛化能力。
- 策略：最小化损失函数。
- 特征选择：递归选择最优特征。
- 生成：对应特征空间的划分，直到所有训练子集被基本正确分类。
- 剪枝：避免过拟合，具有更好的泛化能力。


## 5.3 决策树——特征选择
![[Pasted image 20220516202517.png]]

![[Pasted image 20220516202645.png]]

![[Pasted image 20220516202700.png]]

### 5.3.1 信息增益

- 熵表示的是随机变量不确定性。
$$H(X)=-\sum_{i=1}^np_ilogp_i$$
或
$$H(p)=-\sum_{i=1}^np_ilogp_i$$
- 随机变量的取值等概率分布的时候，相应的熵最大。
$$0\ge H(p)\ge log n$$

- 条件熵：
$$H(Y|X)=-\sum_{i=1}^np_iH(Y|X=x_i)$$
- 当熵和条件熵中的概率由数据估计得到时，则为经验熵和经验条件熵。
- 信息增益：得知特征X而使类Y的信息的不确定性减少的程度。
$$g(D,A)=H(D)-H(D|A)$$

信息增益：算法

输入：训练数据集$D$和特征$A$：
输出：特征$A$对$D$的信息增益$g(D,A)$

- 计算经验熵$H(D):$
$$H(D)=-\sum_{k=1}K\frac{\lvert C_k\rvert}{\lvert D\rvert}log_2\frac{\lvert C_k\rvert}{\lvert D\rvert}$$
- 计算经验条件熵$H(D|A)$:
$$H(D|A)=\sum_{i=1}^n\frac{\lvert D_i\rvert}{\lvert D\rvert}H(D_i)=\sum_{i=1}^n\frac{\lvert D_i\rvert}{\lvert D\rvert}\sum_{k=1}^K\frac{\lvert D_{ik}\rvert}{\lvert D\rvert}log_2\frac{\lvert D_{ik}\rvert}{\lvert D\rvert}$$
- 计算信息增益：
$$g(D,A)=H(D)-H(D|A)$$

### 5.3.2 例题解说

![[Pasted image 20220517095814.png]]

![[Pasted image 20220517100451.png]]

![[Pasted image 20220517102917.png]]


### 5.3.3 信息增益比
![[Pasted image 20220517103216.png]]

不同的特征会由于其不同的取值个数而影响整体的信息增益，所以这里我们引入信息增益比来消除其影响。

![[Pasted image 20220517104934.png]]

![[Pasted image 20220517104952.png]]

上述的信息增益和信息增益比互有优劣。

## 5.4 决策树——决策树的生成
### 5.4.1 算法解说
#### 5.4.1.1 ID3算法
输入：训练数据集$D$、特征集$A$、阈值$\epsilon$
输出：决策树$T$
- 判断$T$是否需要选择特征生成决策树；
    -若$D$中所有实例属于同一类，则T为单节点树，记录实例类别$C_k$,以此作为该结点的类标记，并返回$T$。
    -若$D$中所有实例无任何特征（$A=\emptyset$）,则$T$为单结点树，记录$D$中实例个数最多类别$C_k$，以此作为该结点的类标记，并返回$T$；

- 否则，计算$A$中各特征的**信息增益**，并选择**信息增益**最大的特征$A_g$；
    -若$A_g$的**信息增益**小于$\epsilon$，则$T$为单节点树，记录$D$中实例个数最多类别$C_k$，以此作为该结点的类标记，并返回$T$；
    -否则，按照$A_g$的每个可能值$a_i$，将$D$分为若干非空子集$D_i$，将$D_i$中实例个数最多的类别作为标记，构件子结点，以结点和其子结点构成$T$,并返回$T$；

- 第$i$个子结点，以$D_i$为训练集，$A-A_g$为特征集合，递归地调用以上步骤，得到子树$T_i$并返回。

#### 5.4.1.2 C4.5算法
输入：训练数据集$D$、特征集$A$、阈值$\epsilon$
输出：决策树$T$
- 判断$T$是否需要选择特征生成决策树；
    -若$D$中所有实例属于同一类，则T为单节点树，记录实例类别$C_k$,以此作为该结点的类标记，并返回$T$。
    -若$D$中所有实例无任何特征（$A=\emptyset$）,则$T$为单结点树，记录$D$中实例个数最多类别$C_k$，以此作为该结点的类标记，并返回$T$；

- 否则，计算$A$中各特征的**信息增益比**，并选择**信息增益比**最大的特征$A_g$；
    -若$A_g$的**信息增益**比小于$\epsilon$，则$T$为单节点树，记录$D$中实例个数最多类别$C_k$，以此作为该结点的类标记，并返回$T$；
    -否则，按照$A_g$的每个可能值$a_i$，将$D$分为若干非空子集$D_i$，将$D_i$中实例个数最多的类别作为标记，构件子结点，以结点和其子结点构成$T$,并返回$T$；

- 第$i$个子结点，以$D_i$为训练集，$A-A_g$为特征集合，递归地调用以上步骤，得到子树$T_i$并返回。

利用信息增益比可以消除特征值数量不同的影响，同时$C4.5$算法不仅能够处理离散型的特征，还能处理连续性的特征。


### 5.4.2 例题详解——以ID3算法为例

![[Pasted image 20220517135702.png]]

![[Pasted image 20220517141137.png]]

第一个特征年龄的信息增益：
![[Pasted image 20220517141740.png]]

第二个特征有工作的信息增益：

![[Pasted image 20220517141838.png]]

第三个特征信贷情况的信息增益：

![[Pasted image 20220517141928.png]]

![[Pasted image 20220517142028.png]]

![[Pasted image 20220517142138.png]]

## 5.5 决策树——剪枝
优秀的决策树：
在具有好的拟合和泛化能力的同时，
- 深度小；
- 叶结点少；
- 深度小并且叶结点少

深度：所有结点的最大层次数

![[Pasted image 20220517143326.png]]

- 误差：训练误差、测试误差
- 过拟合：训练误差低，测试误差高（模型结构过于复杂）
- 欠拟合：训练误差高，测试误差低（模型结构过于简单）

- 剪枝：处理决策树的过拟合问题
- 预剪枝：生成过程中，对每个结点划分前进行评估，若当前结点的划分不能提升泛化能力，则停止划分，记当前结点为叶结点；
- 后剪枝：生成一颗完整的决策树之后，自底向上地对内部结点考察，若此内部结点变为叶节点可以提升泛化能力，则做此替换。

### 5.5.1 预剪枝
预剪枝的方法：

- 限定决策树的深度
- 设定一个阈值
- 设置某个指标，比较结点划分前后的泛化能力

例子：挑西瓜

![[Pasted image 20220517150024.png]]

- 限定决策树的深度：

![[Pasted image 20220517150051.png]]

限定深度为2：

![[Pasted image 20220517150412.png]]

- 预剪枝：设定阈值：

![[Pasted image 20220517150520.png]]

![[Pasted image 20220517150630.png]]

- 预剪枝：基于测试集上的误差率：

![[Pasted image 20220517150816.png]]

![[Pasted image 20220517151944.png]]

预剪枝第一层

![[Pasted image 20220517151924.png]]

![[Pasted image 20220517152151.png]]

预剪枝第二层：

![[Pasted image 20220517152405.png]]

### 5.5.2 后剪枝
后剪枝方法：

- 降低错误剪枝（REP）
- 悲观错误剪枝（PEP）
- 最小误差剪枝（MEP）
- 基于错误的剪枝（EBP）
- 代价-复杂度剪枝（CCP）

#### 5.5.2.1 降低错误剪枝（REP）
原理：自下而上，使用测试集来剪枝。对每个结点，计算剪枝前和剪枝后的误判个数，若是剪枝有利于减少误判（包括相等的情况），则减掉该结点所在分枝。

![[Pasted image 20220517153018.png]]

完整的树：

![[Pasted image 20220517154020.png]]

后剪枝：第四层

![[Pasted image 20220517154121.png]]

![[Pasted image 20220517154156.png]]

后剪枝：第三层

![[Pasted image 20220517154221.png]]

![[Pasted image 20220517154319.png]]

后剪枝：第二层

![[Pasted image 20220517170209.png]]

停止剪枝

REP结果：
- 计算复杂性是线性的
- 操作简单，容易理解
- 受测试集影响大，如果测试集比训练集小很多，会限制分类的精度



#### 5.5.2.2 悲观错误剪枝（PEP）
原理：根据剪枝前后的错误率来决定是否剪枝。和REP不同之处在于，PEP只需要训练集即可，不需要验证集，并且PEP是自上而下剪枝的。

具体步骤：

- 计算剪枝前目标子树每个叶子结点的误差，并进行连续修正（可用积分方式避免计算组合数）：
修正原理可见 
$$Error(Leaf_i)=\frac{error(Leaf_i)+0.5}{N(T)}$$
- 计算剪枝前目标子树的修正误差：
$$Error(T)=\sum_{i=1}^LError(Leaf_i)=\frac{\sum_{i=1}^Lerror(Leaf_i)+0.5*L}{N(T)}$$
上述式子中的$L$表示该目标子树叶子结点的个数。
- 计算剪枝前目标子树误判个数的期望值：
$$E(T)=N(T)\times Error(T)=\sum_{i=1}^Lerror(Leaf_i)+0.5*L$$
- 计算剪枝前目标子树误判个数的标准差：
$$std(T)=\sqrt{N(T)\times Error(T)\times (1-Error(T))}$$
- 计算剪枝前误判上限(即悲观误差)：
$$E(T)+std(T)$$
- 计算剪枝后该结点的修正误差：
$$Error(Leaf)=\frac{error(Leaf)+0.5}{N(T)}$$
- 计算剪枝后该结点误判个数的期望值：
$$E(Leaf)=error(Leaf)+0.5$$
- 比较剪枝前后的误判个数，如果满足以下不等式，则剪枝；否则，不剪枝。
$$E(Leaf)\lt E(T)+std(T)$$

上述式子中$N(T)$表示该子树包含的总的样本个数。


例子：

![[Pasted image 20220531204331.png]]


先考虑T4结点：
![[Pasted image 20220531204921.png]]
此时不满足，故不剪枝


考虑T4结点：
![[Pasted image 20220531205151.png]]
此时条件满足，需要剪枝。

西瓜数据集：

![[Pasted image 20220531205337.png]]

判断根蒂结点是否需要剪枝

![[Pasted image 20220531205544.png]]

此时条件满足，故而剪枝

悲观错误剪枝方法：
- 不需要分类剪枝数据集，有利于实例较少的问题
- 误差使用了连续修正值，使得适用性更强
- 由于自上而下的剪枝策略，PEP效率更高
- 可能会修剪掉不应剪掉的枝条

#### 5.5.2.3 最小误差剪枝（MEP）

原理：根据剪枝前后的最小分类错误概率来决定是否剪枝。自下而上剪枝，只需要训练集即可。

- 在结点$T$处，属于类别$k$的概率：
$$P_k(T)=\frac{n_k(T)+P_{r_k}(T)\times m}{N(T)+m}$$

其中，$P_{r_k}(T)$表示在结点$T$处的先验概率，$m$则表示先验概率对于后验概率的影响。

![[Pasted image 20220531210855.png]]

- 在结点$T$处，预测错误率：
$$Error(T)=min\{1-P_k(T)\}=min\{\frac{N(T)-n_k(T)+m(1-P_{r_k}(T))}{N(T)+m}\}$$

此时先验确定，样本集中的每个样本所述类别确定，故而上式所求的即是对应的$m$。


具体步骤：
此处考虑类别先验分布为均匀分布。此时上述式子变为：
$$Error(T)=\frac{N(T)-n_k(T)+K-1}{N(T)+K}$$
步骤如下：

- 计算剪枝前目标子树每个叶子结点的预测错误率：
$$Error(Leaf_i)=\frac{N(Leaf_i)-n_k(Leaf_i)+K-1}{N(Leaf_i)+K}$$
- 计算剪枝前目标子树的预测错误率：
$$Error(Leaf_i)=\sum_{i=1}^Lw_iError(Leaf_i)=\sum_{i=1}^L\frac{N(Leaf_i)}{N(T)}Error(Leaf_i)$$
- 计算剪之后目标子树的预测错误率：
$$Error(T)=\frac{N(T)-n_k(T)+K-1}{N(T)+K}$$
- 比较剪枝前后的预测错误率，如果满足以下不等式，则剪枝；否则，不剪枝。
$$Error(T)\lt Error(Leaf)$$

例子：

![[Pasted image 20220531213503.png]]

![[Pasted image 20220531213722.png]]

此时不剪枝。

#### 5.5.2.4 基于错误的剪枝（EBP）

原理：根据剪枝前后的误判个数来决定是否剪枝。自下而上剪枝，只需要训练集即可。

误判率上界：

置信水平$\alpha$下每个每个节点的误判率上界：
$$U_{\alpha}(e(T),N(T))=\frac{e(T)+0.5+\frac{q_{\alpha}^2}{2}+q_{\alpha}\sqrt{\frac{(e(T)+0.5)(N(T)-e(T)-0.5)}{N(T)}+\frac{q_{\alpha}^2}{4}}}{N(T)+q_{\alpha}^2}$$

![[Pasted image 20220601093957.png]]

![[Pasted image 20220601094321.png]]

使用线性差值求得对应的上分位数

具体步骤：
- 计算剪枝前目标子树每个叶子结点的误判率上界：
$$U_{CF}(e(Leaf_i),N(Leaf_i))=\frac{e(Leaf_i)+0.5+\frac{q_{\alpha}^2}{2}+q_{\alpha}\sqrt{\frac{(e(Leaf_i)+0.5)(N(Leaf_i)-e(Leaf_i)-0.5)}{N(Leaf_i)}+\frac{q_{\alpha}^2}{4}}}{N(Leaf_i)+q_{\alpha}^2}$$
- 计算剪枝前目标子树的误判个数上界：
$$E(Leaf)=\sum_{i=1}^LN(Leaf_i)U_{CF}(e(Leaf_i),N(Leaf_i))$$
- 计算剪枝后目标子树的误判率上界：
$$U_{CF}(e(T),N(T))=\frac{e(T)+0.5+\frac{q_{\alpha}^2}{2}+q_{\alpha}\sqrt{\frac{(e(T)+0.5)(N(T)-e(T)-0.5)}{N(T)}+\frac{q_{\alpha}^2}{4}}}{N(T)+q_{\alpha}^2}$$
- 计算剪枝后目标子树的误判个数上界：
$$E(T)=N(T)U_{CF}(e(T),N(T))$$
- 比较剪枝前后的误判个数上界，如果满足以下不等式，则剪枝；否则，不剪枝。
$$E(T)\lt E(Leaf)$$
例子：

![[Pasted image 20220601100601.png]]

#### 5.5.2.4 代价-复杂度剪枝（CCP）
原理：根据剪枝前后的损失函数来决定是否剪枝。


损失函数：
$$C_{\alpha}=C(T)+\alpha|T|$$
其中，$C(T)=\sum_{t=1}^{|T|}N_tH_t(T)=-\sum_{t=1}^{|T|}\sum_{k=1}^{K}N_{tk}log\frac{N_{tk}}{N_t}$
正则化一般形式：
$$min_{f\in\mathcal{F}}\frac{1}{N}\sum_{i=1}^NL(y_i,f(x_i))+\lambda J(f)$$
![[Pasted image 20220601110155.png]]

具体步骤：
- 剪枝前的决策树记作$T_A$,计算每个叶子结点$t$的经验熵：
$$H_t=-\sum_{k=1}^K\frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$$
- 剪枝前决策树的损失函数为：
$$C_{\alpha}(T_A)=C(T_A)+\alpha|T_A|$$
- 剪枝后的决策树记作$T_B$,损失函数为：
$$C_{\alpha}(T_B)=C(T_B)+\alpha|T_B|$$
- 比较剪枝前后的损失函数，如果满足以下不等式，则剪枝，否则，不剪枝。
$$C_{\alpha}(T_B)\le C_{\alpha}(T_A)$$

## 5.6 CART算法
### 5.6.1 简介
Breiman 1984年提出CART（Classification and Regression Tree）算法，同样需要一个用以选择特征的标准，包括树的生成和剪枝。

![[Pasted image 20220601142444.png]]

![[Pasted image 20220601142916.png]]

### 5.6.2 树的生成
#### 5.6.2.1 分类树
基尼指数：
假设现在有$K$个类，样本点属于第$k$个类的概率为$p_k$,则概率分布的基尼指数为：
$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum_{k=1}^Kp_k^2$$
二分类：
$$Gini(p)=2p(1-p)$$
样本集$D$的基尼指数：
$$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$$
此处的基尼指数类似于熵，用以描述不确定性。值越大，代表其具有更大的不确定性。

给定特征$A$条件下，样本集$D$的基尼指数为：
$$Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)$$
此时默认特征$A$将数据集$D$分为$D_1,D_2$两个部分。

例子：

![[Pasted image 20220601151024.png]]

![[Pasted image 20220601151248.png]]

CART分类树算法：
输入：训练数据集$D$、特征集$A$、阈值$\epsilon$
输出：CART决策树$T$
- 从根节点出发，进行操作，构件二叉树；
- 结点处的训练数据集为$D$,计算现有特征对该数据集的基尼指数，并选择最优特征。
    -在特征$A_g$下，对其可能取的每个值$a_g$，根据样本点对$A_g=a_g$的测试为“是”或“否”，将$D$分割成$D_1$和$D_2$两部分，计算$A_g=a_g$时的基尼指数。
    -选择基尼指数最小的那个值最为该特征下的最优切分点。
    -计算每个特征下的最优切分点，并比较在最优切分下的每个特征的基尼指数，选择基尼指数最小的那个特征，即最优特征。
- 根据最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。
- 分别对两个子结点递归调用上述步骤，直至满足停止条件（比如结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征），即生成CART决策树。

例题解说：

![[Pasted image 20220601154431.png]]

![[Pasted image 20220601154638.png]]

![[Pasted image 20220601155012.png]]

![[Pasted image 20220601155127.png]]

![[Pasted image 20220601160807.png]]

#### 5.6.2.2 回归树
假设将输入空间划分为$M$个单元$R_1,R_2,...,R_M$,并在每个单元$R_m$上有一个固定的输出值$c_m$。回归树模型可表示为：
$$f(x)=\sum_{m=1}^Mc_mI(x\in R_m)$$
平方误差：
$$\sum_{x_i\in R_m}(y_i-f(x_i))^2$$
最优输出：
$$\hat{c}_m=average(y_i|x_i\in Rm)$$

如何划分：
选择第$x^{(j)}$个变量和取值$s$,分别作为切分变量和切分点，并定义两个区域：
$$R_1(j,s)=\{x|x^{(j)}\le s\} ,R_2(j,s)=\{x|x^{(j)}\gt s\}$$
寻找最优切分变量$j$和最优切分点$s$：
$$min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)}(y_i-c_2)^2]$$
对固定输入变量$j$可以找到最优切分点$s$:
$$\hat{c}_1=ave(y_i|x_i\in R_1(j,s)),\hat{c}_2=ave(y_i|x_i\in R_2(j,s))$$

具体步骤：
输入：训练数据集$D$,停止条件
输出：CART决策树$T$
- 从根节点出发，进行操作，构建二叉树；
- 结点处的训练数据集为$D$,计算变量的最优切分点，并选择最优变量。
$$min_{j,s}[min_{c_1}\sum_{x_i\in R_1(j,s)}(y_i-c_1)^2+min_{c_2}\sum_{x_i\in R_2(j,s)(y_i-c_2)^2}]$$
    -在第$j$变量下，对其可能取的每个值$s$，根据样本点分割成$R_1$和$R_2$两部分，计算切分点为$s$时的平方误差。
    -选择平方误差最小的那个值作为该变量下的最优切分点。
    -计算每个变量下的最优切分点，并比较在最优切分下的每个变量的平方误差，选择平方误差最小的那个变量，即最优变量。
- 根据最优特征与最优切分点$(j,s)$，从现结点生成两个子结点，将训练数据集依变量分配到两个子结点中去，得到相应的输出值。
$$R_1(j,s)=\{x|x^{(j)}\le s\},R_2(j,s)=\{x|x^{(j)}\gt s\}
$$
$$\hat{c}_m=\frac{1}{N_m}\sum_{x_i\in R_m(j,s)}y_i,x\in R_m,m=1,2$$
- 继续对两个子区域调用上述步骤，直至满足停止条件，即生成CART决策树。
$$f(x)=\sum_{m=1}^M\hat{c}_mI(x\in R_m)$$

例题讲解：

![[Pasted image 20220602162445.png]]

### 5.6.3 树的剪枝

损失函数：
$$C_{\alpha}(T)=C(T)+\alpha|T|$$
取值：
$$\alpha=\frac{C(t)-C(T_t)}{|T_t|-1}$$
通过交叉验证的方法找到最优的$\alpha$。

CART树的剪枝算法：
输入：CART算法生成的决策树
输出：最优决策树$T_{\alpha}$
(1)设$k=0$，$T=T_0$
(2)设$\alpha=+\infty$
(3)自下而上地对各内部结点$t$计算$C(T_t)$,$|T_t|$
以及
$$g(t)=\frac{C(t)-C(T_t)}{|T_t|-1},\alpha=min(\alpha,g(t))$$
这里，$T_t$表示以$t$为根节点的子树，$C(T_t)$是对训练数据的预测误差，$|T_t|$是$T_t$的叶节点个数。
(4)自上而下地访问内部结点$t$,如果有$g(t)=\alpha$,进行剪枝，并对叶节点$t$以多数表决法决定其类，得到树$T$。
(5)设$k=k+1$,$\alpha_k=\alpha$,$T_k=T$。
(6)如果$T$不是由根结点单独构成的树，则回到步骤(4)。
(7)采用交叉验证法在子树序列$T_0,T_1,...,T_n$中选取最优子树$T_{\alpha}$。
