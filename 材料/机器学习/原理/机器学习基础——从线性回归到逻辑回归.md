
# 机器学习基础——从线性回归到逻辑回归

在之前的文章当中，我们推导了线性回归的公式，线性回归本质是**线性函数**，模型的原理不难，核心是**求解模型参数**的过程。通过对线性回归的推导和学习，我们基本上了解了机器学习模型学习的过程，这是机器学习的精髓，要比单个模型的原理重要得多。


## 1.1 回归与分类

在机器学习当中，模型根据预测结果的不同分为两类，如果我们希望模型预测一个或者多个**连续值**，这类问题被称为是**回归问题**。像是常见的未来股票价格的估计、未来温度估计等等都算是回归问题。还有一类呢是**分类问题**，模型的预测结果是一个**离散值**，也就是说只有固定的种类的结果。常见的有垃圾邮件识别、网页图片鉴黄等等。


我们之前介绍的逻辑回归顾名思义是一个回归问题，今天的文章讲的呢是如何将这个回归模型转化成分类模型，这个由线性回归推导得到的分类模型称为逻辑回归。


## 1.2 逻辑回归

逻辑回归这个模型很神奇，虽然它的本质也是回归，但是它是一个分类模型，并且它的名字当中又包含”回归“两个字，未免让人觉得莫名其妙。

如果是初学者，觉得头晕是正常的，没关系，让我们一点点捋清楚。

让我们先回到线性回归，我们都知道，线性回归当中$y=Wx+b$。我们通过$W$和$b$可以求出$X$对应的$y$,这里的$y$是一个连续值，是一个回归模型对吧。但如果我们希望这个模型来做分类呢，应该怎么办？很简单，我们需要人为地设置阈值对吧，比如我们规定$y \gt 0$
最后的分类是1,$y \lt 0$最后的分类是0.从表面上来看，这当然是可以的，但实际上这样操作会有很多问题。

最大的问题在于如果我们简单地设计一个阈值来做判断，那么会导致最后的y是一个分段函数，而**分段函数不连续**，使得我们没有办法对它求梯度，为了解决这个问题，我们得找到一个**平滑的函数**使得既可以用来做分类，又可以解决**梯度**的问题。

很快，信息学家们找到了这样一个函数，它就是**Sigmoid函数**，它的表达式是：

$$\sigma(x)=\frac{1}{1+e^{-x}}$$

它的函数图像如下：

![[6e0e00f157c05bc59d23b69c0b4a7119.png]]

可以看到，sigmoid函数在$x=0$处取值0.5，在正无穷处极限是1，在负无穷处极限是0，并且函数连续，处处可导。sigmoid的函数值的取值范围是0-1，非常适合用来反映**一个事物发生的概率**。

我们认为$\sigma(x)$表示$x$发生的概念，那么$x$不发生的概念$1-\sigma(x)$。我们把发生和不发生看成是两个类别，那么sigmoid函数就转化成了分类函数，如果$\sigma(x) \gt0.5$ 表示类别1，否则表示类别0。

到这里就很简单了，通过线性回归我们可以得到$y=WX+b$,$P(1)=\sigma(y),P(0)=1-\sigma(y)$。也就是说我们在线性回归模型的外面套了一层sigmoid函数，我们通过计算出不同的$y$,从而获得不同的概率，最后得到不同的分类结果。


## 1.3 损失函数
下面的推导**全程高能**，我相信你们看完会三连的（在看、转发、关注）。

让我们开始吧，我们先来确定一下符号，为了区分，我们把训练样本当中的真实分类命名为$y$,$y$的矩阵写成$Y$。同样，单条样本写成$x$,$x$的矩阵写成$X$。单条预测的结果写成了$y_{pred}$，所有的预测结果的矩阵写成$y_{pred}$。

对于单条样本来说，$y$有两个取值，可能是1，也可能是0,1和0代表两个不同的分类。我们希望$y=1$的时候，$y_{pred}$尽量大，$y=0$时，$1-y_{pred}$尽量大，也就是$y_{pred}$尽量小，因为它取值在0-1之间。我们用一个式子来统一这两种情况：

$$P(y|x)=y_{pred}^y(1-y_{pred})^{1-y}$$
我们代入一下，$y=0$时前项为1，表达式就只剩下后项，同理，$y=1$时，后项为1，只剩下前项。所以这个式子就可以表示预测准确的概率，我们希望这个概率尽量大。显然，$P(y|x) \gt 0$,所以我们可以对它求对数，因为log函数是单调的，所以$P(y|x)$取最值时的取值，就是$logP(y|x)$取最值的取值。

$$logP(y|x)=ylogy_{pred}+(1-y)log(1-y_{pred})$$
我们期望这个值最大，也就是期望它的相反数最小，我们令$J=-logP(y|x)$,这样就得到了它的损失函数：

$$J(\theta)=-\frac{1}{m}YlogY_{pred}+(1-Y)log(1-Y_{pred})$$
如果知道交叉熵这个概念的同学，会发现这个损失函数的表达式其实就是**交叉熵**。

交叉熵是用来衡量两个概率分布之间的”**距离**“，交叉熵越小说明两个概率分布越接近，所以经常被用来当做分类模型的**损失函数**。关于交叉熵的概念我们这里不多赘述，会在之后文章当中详细介绍。我们随手推导的损失函数刚好就是交叉熵，这并不是巧合，其实底层是有一套**信息论**的数学逻辑支撑的，我们不多做延伸，感兴趣的同学可以了解一下。

## 1.4 硬核推导
损失函数有了，接下来就是求梯度来实现梯度下降了。

这个函数看起来非常复杂，要对它直接求偏导**过于硬核**（危），如果是许久不碰高数的同学直接肝不亚于硬抗苇名一心。

为了简化难度，我们先来做一些准备工作。首先，我们先来看下$\sigma$ 函数，它本身的形式很复杂，我们先把它的导数搞定。

![[Pasted image 20220507104444.png]]

因为$y_{pred}=\sigma(\theta X)$ ,我们将它带入损失函数，可以得到：
![[Pasted image 20220507104632.png]]

接着我们$J(\theta)$求对$\theta$的偏导，这里要代入上面对$\sigma(x)$求导的结论：

![[Pasted image 20220507104812.png]]