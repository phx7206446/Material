# 1. “基尼指数”在经济学中的奥秘

## 1.1 基尼指数的起源

早在春秋战国时期，孔子曾在《论语》中指出：“不患寡而患不均，不患贫而患不安”。可是该怎么去度量这个**「均」**呢？可能很多小伙伴已经瞬间想到算个平均值吧？但是，即使可以算出个平均值，又怎么来度量分配不均的程度呢？

### 1.1.1 洛伦茨曲线

**「1905年」**，美国有位奥地利统计学家**「洛伦茨」**想到了一个办法，他准备画一条曲线。这条曲线很简单，横坐标是**「人口」**，纵坐标是**「收入」**。

![[33c61bbea95b76ced792bd71716d5384.jpg]]

- 首先将全社会的人口按照收入由低至高排序，并分成若干份。这里以5份为例。
- 然后分别计算累计20%，40%，60%和80%的人口收入占社会总收入的百分比，一次描出点$E_1、E_2、E_3$和$E_4$。
- 接着把这5个点连接起来就生成了洛伦茨曲线$O-E_1-E_2-E_3-E_4-L$

那么这条曲线又说明了什么呢？
- $E_1$-全社会的20%人口占有总收入的5%
- $E_2$-全社会的40%人口占有总收入的10%
- $E_3$-全社会的60%人口占有总收入的30%
- $E_4$-全社会的80%人口占有总收入的50%

这说明剩余的20%人口竟然占有着总收入的50%。这大概是什么意思呢？

举个栗子，假设有10个人，大家一天能赚的总收入是100元。那么同样一天里，2个人平均赚25元，4个人平均赚10元，而另外4个人只能平均赚2块5!

**「这条曲线越弯曲，距离OL那条线越远，越代表着收入分配不平等」**

换而言之，中间黄色部分的面积越小，浅黄色的部分面积越大，则分配越平等。

可是光看曲线只能得到一个大致的印象呀，能否用数据说话呢？

### 1.1.2 基尼指数
基尼将洛伦茨曲线由多点折线图改进成平滑曲线，并引入数学统计思维，提出一个数值指标，是不是直接从定性分析到了定量分析啦？

那么怎么做定量计算呢？

还是刚才那个例子。

此处，$A$和$B$的面积我们用积分进行计算，就可以得到一个简单的基尼指数。

已知
$$x=[0,20,40,60,80,.100]$$
$$y=[0,5,10,30,50,100]$$
求出拟合函数的解析式，此过程我们用Python来实现。

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

# 设置语言
matplotlib.rcParams['font.family']='STSong'

matplotlib.rcParams['font.size']=14

# 数据准备
x=np.array([0,20,40,60,80,100])
y=np.array([0,5,10,30,50,100])
deg=np.array([2,3,4,5])
N=20000 #拟合曲线的点数量

# 定义拟合函数
def curve(x,y,deg,N):
    ''' 
    Parameters  
    ----------  
    deg : TYPE 多项式拟合的最高幂次  
    N : TYPE 返回绘图拟合的点的数量  
    Returns  
    -------  
    fit_curve : TYPE 对输入X的拟合结果  
    draw_x : TYPE 对输入X变换后的，用作画图的X  
    draw_y : TYPE 对输入X变换后的，用作画图的y  
    '''
    draw_x=np.linspace(x.min(),x.max(),N)
    parameter=np.polyfit(x,y,deg)
    coefficient=np.poly1d(parameter)
    fit_curve=np.array(cofficient(x))
    draw_y=np.array(coefficient(draw_x))
    return fit_curve,draw_x,draw_y

# 定义计算可决系数函数
def coe_of_determination(y,fit_curve):
    '''
    Parameters
    ----------  
    y : TYPE 原本的y  
    fit_curve : TYPE 拟合后的y  
    Returns  
    -------  
    R^2,可决系数  
    '''
    return (np.array(round(np.corrcoef(y,fit_curve)[0,1]**2,3)))

# 分别计算deg的拟合曲线  
fit_curves = np.empty(shape = (len(deg),len(x)))  
draw_xs = np.empty(shape = (len(deg),N))  
draw_ys = np.empty(shape = (len(deg),N))  
for i in range(len(deg)):  
    fit_curves[i,:]= curve(x,y,deg[i],N)[0]  
    draw_xs[i,:]= curve(x,y,deg[i],N)[1]  
    draw_ys[i,:]= curve(x,y,deg[i],N)[2]  
  
# 计算可决系数  
coe = np.empty(len(fit_curves))  
for i in range(len(fit_curves)):  
    coe[i] = coe_of_determination(y,fit_curves[i,:])  
  
# 绘制图形  
plt.scatter(x, y)  
for i in range(len(fit_curves)):  
    plt.plot(draw_xs[0,:], draw_ys[i,:])  
plt.legend(coe.tolist())  
plt.xlabel('累计人口占比',fontproperties='STSong',fontsize=10)  
plt.ylabel('累计收入占比',fontproperties='STSong',fontsize=10)  
plt.show()
```

接下来调用Curve_fitting（x,y,deg）函数，因为有6个坐标点，因此分别尝试了$2、3、4、5$为拟合多项式次数，对应的$R^2$分别是$0.987、0.996、0.997、1$。

![[e324db064c879144d064c142c1302fc0.png]]

从图可以看出， $5$次多项式虽然可以完美的通过所有的已知点，但显然是过拟合的，根据奥卡姆剃刀原理，我们其实取个$3$次多项式曲线就可以了。

这样，我们就成功把一个定性的描述转为定量的数据分析了，接着根据面积积分计算出的基尼指数为$0.4417$。

## 1.2 基尼指数的应用
### 1.2.1 基尼指数能说明什么呢？
那么这个$0.4417$说明什么问题呢？

> 基尼指数，反映居民之间贫富差异程度的常用统计指标，较全面客观地反映居民之间的贫富差距，能预报、预警居民之间出现贫富两极分化。——国家统计局

这就是说，我们可以通过基尼指数来定量反映某地区的贫富差距，以便政府做出相应的决策。

### 1.2.2 基尼指数多大合适呢？
国际上认为基尼指数合理的范围为0.3-0.4，太小了不好，代表社会收入差距太小，干多干少都一样，久而久之人们会越来越懒，社会也就没有了活力；基尼指数太大也不好，太大就会”四海无闲田，农夫犹饿死“，造成社会的极不稳定。

上面例子中算出来的$0.4417$ ，对照于下表可以看出指数等级高，差距较大。
- $\lt 0.2$表示指数等级极低（高度平均）
- $0.2-0.29$表示指数等级低（比较平均）
- $0.3-0.39$表示指数等级中（相对合理）
- $0.4-0.59$表示指数等级高（差距较大）
- $\gt 0.6$表示指数等级极高（差距悬殊）

在基尼指数中，通常把$0.4$作为收入分配差距的“警戒线”，根据黄金分割率，其准确值应为$0.382$。一般发达国家的基尼指数在$0.24$到$0.36$之间。

![[58319ec6fcb743f4f50eeee5c7a54a77.png]]

### 1.2.3 基尼指数的分类和计算
基尼指数按照用途还会分为收入基尼指数、财富基尼指数、消费基尼指数。

同样，学术界有很多种计算基尼指数的方法，比如几何方法、基尼的平均差方法、协方差方法、矩阵方法等等，各种计算方法之间具有统一性。计算基尼指数有时以离散分布为基础，有时以连续分布为基础，虽然其数学表达不一样，但其内涵是统一的。

按照国家统计局公布的方法，计算基尼指数的公式如下：
$$Gini=1-\sum_{i=1}^np_i(2Q_i-w_i)$$
其中$w_i$表示第$i$组收入占总收入的比例，$p_i$表示第$i$组人口占总人口比例，并且
$$Q_i=\sum_{k=1}^iw_k$$
等一下，这个公式和基尼指数的面积公式有何联系呢？
![[55bcfee0ac7fa21048e067b5f9605c14.png]]

一脸懵的你不要着急，我们一起来看一下数学的Magic就明白了。
$$Gini=\frac{S_A}{S_A+S_B}=1-\frac{S_B}{S_A+S_B}$$
因为$S_A+S_B$为正方形的一半$\frac{1}{2}$,所以$Gini=1-2S_B$

现在我们只需要求一下$B$区域的面积就可以啦，假如我们把整个区域分为$n$小份：
$$2S_B=2\sum_{i=1}^nS_i$$
利用以直代曲的思想，红色阴影部分的面积近似为梯形，左边那个边就是前$i-1$个$w_k$的和，右边那个自然就是前$i$个$w_k$的和啦，每一小份的占比是$p_i$,于是：
$$S_i=\frac{p_i(\sum_{k=1}^{i-1}w_k+\sum_{k=1}^iw_k)}{2}$$
那么
$$\begin{aligned}
Gini&=1-\sum_{i=1}^np_i(\sum_{k=1}^{i-1}w_k+\sum_{k=1}^iw_k)\\
&=1-\sum_{i=1}^np_i(2\sum_{k=1}^iw_k-w_i)\\
&=1-\sum_{i=1}^np_i(2Q_i-w_i)
\end{aligned}$$
这个公式推导的精华在于，**「结合几何面积的特点」**对数据进行了化简，怎么样，是不是也没有那么难呢？

好啦，关于经济中的基尼指数就先讲到这里啦。下一节中我们将聊聊机器学习中常用的那个。

# 2.“基尼指数”在机器学习中的奥秘

在上一节中，我们介绍了基尼指数在经济学领域的作用，其含义为：
$$Gini=\frac{A}{A+B}$$
![[微信图片_20220603100224.png]]


经济学中的基尼指数衡量着收入分配的均衡程度，然而在机器学习中也有一个基尼指数，两者虽说名字很像，但是内核却大不同。机器学习中虽然也称作基尼指数（Gini Index），可是它度量的是数据集的纯度，所以也称作基尼不纯度（Gini Impurity）。

## 2.1 什么是基尼不纯度？

> Gini Impurity是衡量能否把大量随机变量进行有效分类的指标，如果分的不纯，那么数值就打，如果分的够清，那么数值就小。

那么这有什么用呢？它又是谁提出的呢？

基尼不纯度的定义如下：

假设有$K$个类，样本点属于第$k$类的概率为$p_k$，那么概率分布的基尼不纯度定义为：
$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum p_k^2$$
显然，这就是样本点被错分的概率期望。如果整个样本集只有一个类别，那么基尼不纯度就是0，表示样本集纯度达到最高值。反正总共就一个类，那么任意抽取一个样本，自然就知道它的归属类别啦。

## 2.2 基尼不纯度有啥特点
那么这个基尼不纯度有啥特点呢？我们来好好欣赏一下这个公式。

1. 对于二分类问题

对于二类分类问题来说，如果样本点属于第一类的概率是$p$，那么显而易见第二类的概率就是$1-p$啦，带入到这个公式里就是：
$$
\begin{aligned}
Gini(p)&=p(1-p)+(1-p)(1-(1-p))\\
&=2p(1-p)
\end{aligned}
$$
如果对给定的样本集合$D$,可以分为两个子集$C_1$和$C_2$:
$$Gini(D)=1-\sum_{k=1}^2(\frac{|C_k|}{|D|})^2$$
其中，$\frac{|C_1|}{D}$就是$p$的经验值。

之所以单独把二分类的情况列出来，是因为在提出基尼不纯度的CART算法中用的就是这个，毕竟CART算法生成的是二叉决策树。但其实基尼不纯度完全可以用到多分类问题中，请跟我一起往下看。

2. 对于多分类问题
$$Gini(p)=\sum_{k=1}^Kp_k(1-p_k)=1-\sum p_k^2$$
对于多分类问题，如果对给定的样本集合$D$,可以分为$K$个子集：$C_1,C_2,...,C_K$,其基尼不纯度为：
$$Gini(D)=1-\sum_{k=1}^K(\frac{|C_k|}{|D|})^2$$
其中，$\frac{|C_k|}{D}$就是$p_k$的经验值。

好啦，公式稍稍感受一下即可，接下来看个例子，就完全明白基尼不纯度到底是怎么一回事儿啦！
![[微信图片_20220603113408.png]]


Case 1: 一盒中有10块巧克力，其中黑巧克力有4块，白巧克力有6块，计算得出：
]]$$\begin{aligned}
Gini(Case1)&=1-(\frac{4}{10})^2-(\frac{6}{10})^2\\
&=0.48
\end{aligned}
$$
![[微信图片_20220603113328.png]]


Case 2:一盒中有10块巧克力，其中黑巧克力3块，白巧克力2块，草莓味巧克力1块，松露巧克力4块，计算得出：
$$
\begin{aligned}
Gini(Case2)&=1-(\frac{3}{10})^2-(\frac{2}{10})^2-(\frac{1}{10})^2-(\frac{4}{10})^2\\
&=0.7
\end{aligned}
$$

See?看出来什么特点了吗？没错，求出来的$Gini$指数越大，说明闭眼随便拿一个巧克力口味的不确定性就越大，对于$Case2$来说，随机性就越强。

好吧，你可能会说这是一个很明显的事呀，那知道这个公式的特点了又有什么用呢？别着急，继续往下看。

假如我们只考虑有两个类别的情况，但是样本还具有其他的属性特征，是否**「可以借助其他属性特征增加分类的确定性」**呢？

很简单，如果样本集合$D$根据特征$A$的取值$a_1,a_2,a_3,...,a_m$,将$D$分为$m$个子集$D_1,D_2,...,D_m$,每个子集都可以计算出一个基尼不纯度，然后以每个子集在$D$的占比$\frac{|D_i|}{|D|}$作为权重，进行加权求和，即得到在特征$A$的条件下集合$D$的基尼不纯度定义：
$$Gini(D;A)=\sum_{i=1}^m\frac{D_i}{D}Gini(D_i)$$
这样在特征$A$的加持下，**「能增加样本分类的确定性」**，逐一遍历，即可得到可以使分类更准确的决策。

## 2.4 基尼不纯度有何应用
还记得泰坦尼克号中凄美的故事吗？爱情是美丽的，结局是悲惨的，数据是冷酷的。据数据统计，这个事件中约2/3的人在海滩中丧生。
![[微信图片_20220603113437.png]]
根据记载，当时船上有1316名乘客和892名船员，共2208人，事故发生后幸存了718人。2208人中，分别按照性别、年龄和舱位三个特征进行了统计：

1. 性别
|  性别   | 丧失人数  | 幸存人数 |合计|
|  ----  | ----  |----|----|
| 男 | 1364 |374|1738|
| 女 | 126 |344|470|

2. 年龄
|  年龄   | 丧失人数  | 幸存人数 |合计|
|  ----  | ----  |----|----|
| 成人 | 1438 | 661 | 2099 |
| 儿童 | 52 | 57 | 109 |

3. 舱位
|  年龄   | 丧失人数  | 幸存人数 |合计|
|  ----  | ----  |----|----|
| 一等舱 | 122 | 203 | 325 |
| 二等舱 | 167 | 118 | 285 |
| 三等舱 | 528 | 178 | 706 |
| 船员舱 | 673 | 219 | 892 |

如果我们根据上面的冰冷数据，想要得出一个最佳分类点，该怎么计算呢？

可以清晰看出，根据现在的数据，我们能对每个特征构建一个二叉决策树，我们**把性别、年龄、舱位特征分别表示为$A_1$ 、$A_2$ 、$A_3$，并且以$1$ 、$2$ 表示性别的男和女，以 $1$、$2$ 表示年龄的成年和儿童，以 $1$、$2$、$3$、$5$ 表示舱位的一等、二等、三等和船员舱****，**我们把这些数代入到上面那个公式中，来求出各个特征对应的基尼不纯度：

Round 1 求特征$A_1$的基尼不纯度：

记$A_1=1$对应的是子集$D_1$,$A_1=2$对应的是子集$D_2$：
$$\begin{aligned}
Gini(D_1)&=2\times\frac{374}{1738}\times(1-\frac{374}{1738})=0.3378\\
Gini(D_2)&=2\times\frac{344}{470}\times(1-\frac{344}{470})=0.3924\\
Gini(D,A_1)&=\frac{1738}{2208}Gini(D_1)+\frac{470}{2208}Gini(D_2)=0.3494
\end{aligned}
$$

同理可得其他的特征不纯度:
$$
\begin{aligned}
Gini(D,A_2)&=0.4439\\
Gini(D,A_3)&=0.4019
\end{aligned}
$$
从上面三个特征中可以看出来，**「年龄这个特征的基尼不纯度的值最小」**，意味着这个特征的**「确定性更强」**。我们可以优先选择这个特征来进行决策。

这里，大家就发现了，怎么基尼不纯度也能干得了信息熵的工作呢？追其原因，基尼不纯度是熵的泰勒近似，是不是很有趣？
$$-plogp=-plog[1-(1-p)]\approx -p[-(1-p)]=p(1-p)$$

