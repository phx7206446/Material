# 机器学习基础——推导线性回归公式

在之前的文章当中，我们介绍过了简单的朴素贝叶斯分类模型，介绍过**最小二乘法**，所以这期文章我们顺水推舟，来讲讲**线性回归**模型。

线性回归的本质其实是一种统计学当中的回归分析方法，考察的是自变量和因变量之间的线性关联。

后来也许是建模的过程和模型训练的方式和机器学习的理念比较接近，所以近年来，这个模型被归入到了机器学习的领域当中。然而，不管它属于哪个领域，整个模型的思想并没有发生变化。我们只要有所了解即可。



## 1.1 概念模型
线性回归的定义非常简单，它最简单的形式其实就是一元一次方程组。比如，我们有如下式子：

$$y=wx+b$$
我们知道若干的 $x$ 和 $y$ ，要求 $w$ 和 $b$ 。解的方法很简单，我们通过消元法，就可以很容易求出来 $w$ 和 $b$。

我们针对以上的式子做两个变形，第一个变形是我们的自变量$x$不再是一个单值，而是一个m * n的矩阵。$m$表示样本数，$n$表示特征数，我们写成$X$。$X$矩阵的每一行是一个$n$维的行向量，它代表一个样本。它的系数$W$也不再是一个值，而是一个$n * 1$的列向量，它的每一维代表一个样本当中这一维的权重。我们把上面的公式写成矩阵相乘的形式：

$$Y=XW+b$$
式子里的$Y$、$X$和$W$分别是$m * 1$, $m * n$和$n*1$的矩阵。

这里有两点要注意，第一点是这里的b我们可以当做是一个浮点数的参数，但是实际上它也是一个**m * 1的矩阵**（列向量）。但即使我们用的是浮点数也没关系，因为在我们实现模型的时候，numpy或者TensorFlow或者是其他的框架会自动地使用**广播**将它转化成向量来做加法。

第二点是这里的X写在了W的前面，这也是**为了矩阵乘法计算方便**。当然我们也可以将X和W都转置，写成WX，但这样得到的结果是一个1 * m的行向量，如果要和Y进行比较，那么还需要再进行一次转置。所以为了简便，我们对调了X和W的顺序。所以大家不要觉得疑惑，明明是WX+b怎么写出来就成了XW+b了。

我们把式子列出来之后，目标就很明确了，就是**要通过计算求到一个W和b使得式子成立**。但是在此之前，我们需要先明确一点：在实际的工程应用场景当中，是不可能找到W和b使得XW+b恰好和Y完全相等的。因为**真实的场景当中数据都存在误差**，所以精确的解是不存在的，我们只能退而求其次，**追求尽可能精确的解。**


## 1.2 最小二乘法与均方差
在之前的文章当中我们介绍过**最小二乘法**。

在机器学习的过程当中，模型不是直接达到最佳的，而是**通过一步一步的迭代**，效果逐渐提高，最终收敛不再剧烈变化。我们明白了这个过程，就能理解，在学习的过程当中，我们需要一个量化的指标来衡量模型当前学习到的能力。就好像学生在上学的时候需要考试来测试学生的能力一样，我们也**需要一个指标来测试模型的能力**。


对于回归模型而言，预测的目标是一个具体的值。显然这个预测值和真实值越接近越好。我们**假设预测值是** $y_{pred}$ ,真实值是 $y$, 显然应该是 $\lvert y-y_{pred} \rvert$ 越小越好。

但是绝对值的计算非常麻烦，也不方便求导，所以我们通常会将它平方，即： $(y-y_{pred})^2$ 最小。 对于 $m$ 个样本而言，我们希望它们的平方和尽量小： $\sum_{i=1}^m(y_i-y_pred_i)^2$ 。

这个式子和我们之前介绍的方差非常相似，只不过在方差当中减的是真实值。所以这个平方差也有一个类似的名称，叫做均方差。

方差反应的是变量在期望值附近的震荡程度，同样的，**均方差反应的是模型预测值距离真实值的震荡程度**。

寻找最佳的参数来使得均方差尽量小，就是**最小二乘法**。


## 1.3 推导过程

到这里，我们已经搞清楚了模型的公式，也写出了优化的目标，已经非常接近了，只剩下最后一步，就是**优化这个目标的方法**。

如果我们观察一下均方差，我们把它写全： $(Y-(XW+b))^2$
。我们将$W$视作变量的话，这其实是一个**广义的二次函数**。二次函数怎么求最小值？当然是求导了。

在求导之前，我们先对均方差做一个简单的变形：我们想办法把$b$处理掉，让式子尽可能简洁。

首先，我们在$X$当中增加一列1，也就是将$X$变成$m * (n+1)$的矩阵，它的第一列是常数1，新的矩阵写成$X_{new}$ 。

同样，我们在$W$中也增加一行，它的第一行写成$b$，我们将新的矩阵写成 $\theta$，我们可以得到：

$$XW+b=X_{new}\theta$$

之后，我们对均方差进行变形：

$$J(\theta)=\frac{1}{2m}\sum_{i=1}^m(x_i\theta-y_i)^2$$
这里的$m$是样本的数量，是一个常数，我们对均方差乘上这个系数并不会影响$\theta$的取值。这个$J(\theta)$就是我们常说的模型的**损失函数**。

这里的损失其实是误差的意思，损失函数的值越小，说明模型的误差越小，和真实结果越接近。

我们计算$J(\theta)$ 对 $\theta$ 的导数：

$$\frac{\partial J(\theta)}{\partial \theta}=\frac{1}{m}X^T(X\theta-Y)$$

我们令导数等于0，由于$m$ 是常数，可以消掉，得到：
$$\begin{aligned} X^T(X\theta-Y)&=0\\
X^TX\theta&=X^TY\\\theta&=(X^\theta X)^{-1}X^TY\end{aligned}$$


上面的推导过程初看可能觉得复杂，但实际上并不困难。只是一个**简单的求偏导**的推导，我们就可以写出最优的$\theta$的取值。



但实际上，有一点我必须要澄清，虽然上面的代码只有几行，非常方便，但是**在实际使用线性回归的场景当中，我们并不会直接通过公式来计算** $\theta$，而是会使用其他的方法迭代来优化。

那么，我们为什么不直接计算，而要绕一圈用其他方法呢？

原因也很简单，第一个原因是我们计算 $\theta$的公式当中用到了**逆矩阵**的操作。在之前线性代数的文章当中我们曾经说过，只有满秩矩阵才有逆矩阵。如果$X^TX$是**奇异矩阵**，那么它是没有逆矩阵的，自然这个公式也用不了了。

当然这个问题并不是不能解决的，$X^TX$是奇异矩阵的条件是矩阵$X$当中存在一行或者一列全为0。我们通过特征预处理，是可以避免这样的事情发生的。所以样本无法计算逆矩阵只是原因之一，并不是最关键的问题。

最关键的问题是**复杂度**，虽然我们看起来上面核心的代码只有一行，但实际上由于我们用到了逆矩阵的计算，它背后的开销非常大，$X^TX$的结果是一个n * n的矩阵，这里的n是特征的维度。这样一个矩阵计算逆矩阵的复杂度大概在$n^{2.4}$到$n^3$之间。随着我们使用特征的增加，整个过程的复杂度以指数级增长，并且很多时候我们的样本数量也非常大，我们**计算矩阵乘法也有很大的开销。**

正是因为以上这些原因，所以通常我们并不会使用直接通过公式计算的方法来求模型的参数。



