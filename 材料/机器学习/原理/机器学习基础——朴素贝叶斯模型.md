在这篇文章中，主要介绍机器学习领域非常经典的模型——朴素贝叶斯。

# 1.模型原理
朴素贝叶斯，顾名思义和贝叶斯定理肯定高度相关。在这里我们简单回顾一下贝叶斯公式：

$$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$$

我们把$P(A)$和$P(B)$当做先验概率，那么贝叶斯公式就是通过先验和条件概率推算后验概率的公式。也就是寻果溯因，我们根据已经发生的事件去探究导致事件发生的原因。而朴素贝叶斯模型正是基于这个原理，它的原理非常朴素，朴素到一句话就可以概率：**当一个样本有可能属于多个类别的时候，我们简单地选择其中概率最大的那个**。

所以，既然是选择样本所属的类别，显然朴素贝叶斯模型是一个**分类算法**。

在我们具体介绍算法原理之前，我们先来熟悉几个概念。其中几个概念在我们之前的文章当中也介绍过，这里就当做复习。


## 1.1 先验概率
先验概率其实很好理解，我们先不管里面“先后”这两个字。说白了，其实先验概率就是我们可以**事先通过做实验计算的概率**。比如抛硬币正面朝上，比如在一个路口遇到红灯，再比如明天会下雨。

这些事情，有些是我们可以通过实验得到的，有些是可以根据之前的经验估计的。在我们问题当中，这些事件的概率是相对明确的。可以认为是我们**在做模型探究之前就可以确定的概率**，所以称为先验概率。


## 1.2 后验概率
后验概率从直观上来看与先验概率相反，是我们**通过实验或者是之前的经验没有办法直接获取的**。它更多的指的是某个事件由于某个原因或者是另一个事件导致的概率。

举个例子来说，一个学生参加考试，能够及格的概率是可以测量的。无论通过一个学生多次考试进行测试，还是批量学生进行统计，都是可行的。但假设学生在考试之前可以选择复习或者是打游戏，显然，复习会提升学生通过的概率，打游戏可能会降低也可能变化不大，我们不得而知。假设我们知道小明已经通过了考试，想要知道他在考试之前有没有复习，这就是一个后验概率。

从逻辑上来看，**它和条件概率恰好相反**。条件概率是事件A发生的前提下会发生事件B的概率，而后验概率是已经知道事件B发生了，求事件A发生的概率。

## 1.3 似然估计
似然的英文是**likelihood**，从语义上来说它和概率(probability)非常接近，可能只是翻译的时候做了区分。两者在数学公式上的表示也非常接近，都可以写成$P(x|\theta)$。

其中概率求的是已经知道参数$\theta$，事件$x$发生的概率。而似然侧重**事件A发生时的参数**$\theta$。那么自然，似然估计函数就是通过概率分布估计参数的函数了。最大似然估计也就好理解了，就是求**事件A发生时，最有可能的参数**$\theta$**的值**。

举个很简单的例子，假设我们有一个不透明的黑箱，里面有若干个黑球和若干个白球。但我们不知道到底黑球有几个白球有几个。为了探索这个比例，我们有放回地从箱子当中取出10个球，假设最终结果是7黑3白，那么请问箱子里黑球的比例是多少？

这题简直不能更简单，不是小学生的问题么？既然取了10次里面有7个黑球，那显然黑球的概率应该是70%啊，这有什么问题吗？

  
表面上当然毫无问题，但实际上不对。因为我们实验得到的实验结果并不代表概率本身，简单来说，箱子里黑球是70%可以出现7黑3白，箱子里黑球是50%也一样可以出现这个结果，我们怎么能判断箱子里黑球一定是70%呢？

  
这个时候就要用到**似然函数**了。


## 1.4 似然函数
我们把刚才黑白球的实验代入到上面的似然估计的公式当中去，实验最后得到的结果是确定的，是事件$x$。我们要求的，也就是**黑球的比例是参数**$\theta$。由于我们是有放回的实验，所以每次拿出黑球的概率是不变的，根据二项分布，我们可以写出事件$x$发生的概率：

$$P(x|\theta)=\theta^2*(1-\theta)^3=f(\theta)$$
这个式子就是我们的似然函数，也叫**概率函数**。它反映不同的参数下，事件$x$发生的概率。我们要做的就是**根据这个函数计算出**$f(\theta)$**最大时**$\theta$**的取值**。

这个计算过程就很简单了，我们对$\theta$求导，然后令导数等于0，然后求出此时对应的$\theta$的取值。最后的结果当然是$\theta=0.7$时方程有最大值。

我们也可以把$f(\theta)$的函数图像画出来，直观地感受概率分布。

```python
import numpy as np  
import matplotlib.pyplot as plt  
  
x = np.linspace(0, 1, 100)  
y = np.power(x, 7) * np.power(1 - x, 3)  
  
plt.plot(x, y)  
plt.xlabel('value of theta')  
plt.ylabel('value of f(theta)')  
plt.show()
```

![[1f2b8ff7d297c24d6ee272b2f4f504ff.png]]

这也就证明了，我们直观的感受是对的。**不是因为我们拿出来黑球的概率是70%箱子里黑球的比例就是70%，而是箱子里黑球比例是70%拿出来黑球占70%的概率最大。**


## 1.5 模型详解
接下来就到了重头戏，我们还是**先看贝叶斯公式**：

$$P(A|B)=\frac{P(A)P(B|A)}{P(B)}$$
我们接下来对公式进行一个变形，我们假设与$B$事件有关的所有事件的集合为$C$。显然$A \in C$,假设$C$集合中一个有$m$个事件，分别写成:$C_1,C_2,...,C_m$。

我们在追寻事件$B$发生的原因的时候，会追寻出所有可能导致这个结果的参数集合$C$，然后从其中挑选出概率最大的那个作为结果。

我们用它来分类的原理也是一样，对于一个样本$x$，我们会计算出它分别属于所有类别的概率，然后选择其中概率最大的一个作为最终预测的类别。这个朴素的思想就是**朴素贝叶斯模型的原理**。

我们假设$x=\{a_1,a_2,...,a_n\}$,其中的每一个$a$表示样本$x$的一个维度的特征。同样，我们还会有一个类别的集合$C=\{y_1,y_2,...,y_m\}$,其中的每一个y表示一个特定的类别。我们要做的就是计算出x属于各个类别y的概率，选择其中概率最大的那个作为最终的分类结果。

我们根据贝叶斯公式写出概率公式：

$$P(y_i|x)=\frac{P(x|y_i)P(y_i)}{P(x)}$$
其中$P(x)$是一个常量，对于所有的 $i$ 保持不变，所以可以忽略，我们只需要关注分子的部分。

这里，我们做一个重要的假设：我们假设样本 $x$ 中各个维度的特征值彼此是独立的。

这个假设非常重要，如果没有这个假设，那么这里的概率会复杂到我们几乎无法计算。

有了这个假设之后就好办了，我们把公式展开就行：

$$P(y_i|x)=P(y_i)P(a_1|y_i)P(a_2|y_i)...P(a_n|y_i)=P(y_i)\prod_{j=1}^nP(a_j|y_i)$$

其中$P(y_i)$是先验概率，我们可以通过实验或者是其他方法得到，像是$P(a_j|y_i)$就不能直接得到了，就需要我们用统计的方法来计算。

如果 $j$ 是离散值,很简单，我们只要统计 $i$ 事件发生时，各个 $j$ 的实现比例即可。假设我们实验了若干次， $i$ 一共发生了 $M$ 次，$j$ 一共发生了N次，那么显然：

$$P(a_j|y_i)=\frac{N}{M}$$
为了防止M=0，我们可以在分子和分母上同时加上一个平滑参数，所以最终的结果写成：

$$P(a_j|y_i)=\frac{N+\alpha}{M+\beta}$$

但如果 $j$ 是连续值，那么它的取值可能是无数多种，那么显然，我们不可能针对它的每一种取值都去计算概率，也不可能搜集这么多样本。在这种情况下我们应该怎么办呢？

连续值也没关系，我们可以**假设变量的分布满足正态分布**，它的**正态分布曲线其实就是这个变量的概率分布**。

 

用上图举个例子，我们观察最下面的累积百分比这个值。它其实代表x的位置与负无穷之间隔成的区域的面积。这个面积的取值范围是0-1，我们就可以用这个面积的值来代表f(x)的概率。实际上假设变量服从不同维度的正态分布，其实就是**高斯混合模型**（GMM）的思想了，这里点到为止，不做过多展开。

也就是说，如果是离散值，那么我们就通过计算比例的方式来代表概率，如果是连续值，那么就通过正态分布计算概率分布的方法来计算概率。通过这种方法，我们就可以通过 $n$ 个 $P(a_j|y_i)$ 连乘得到 $P(y_i|x)$ 的概率，最后我们比较所有 $y$ 对应的概率。选择其中最大的那个作为分类结果。

以上流程完全正确，但是还**存在一个小小的问题**。

$P(a_j|y_i)$ 是一个浮点数，而且很可能非常小，而我们需要计算 $n$ 个浮点数的乘积。由于存在精度误差，所以当连乘的结果小于精度的时候，就无法比较两个概率之间的大小了。


为了解决上述这个问题，我们需要对浮点数的连乘做一个变形：我们对等式的左右两边去 log。将若干个浮点数相乘，转化成相加：

$$log(P(y_i|x))=log(P(y_i)+log(P(a_1|y_i)+...+log(P(a_n|y_i）=log(P(y_i))+\sum_{i=1}^nlog(P(a_i|yi))$$

由于对数函数是**单调函数**，所以我们可以直接用取完对数之后的结果来比大小，就可以避免精度带来的影响了。

# 2. 代码实战
在上一节中，我们介绍了朴素贝叶斯模型的基本原理。

朴素贝叶斯的核心本质是假设样本当中的变量服从某个分布，从而利用条件概率计算出样本属于某个类别的概率。一般来说一个样本往往会含有许多特征，这些特征之间很有可能是有相关性的。为了简化模型，朴素贝叶斯模型假设**这些变量是独立的**。这样我们就可以很简单地计算出样本的概率。


在我们学习算法的过程中，如果只看模型的原理以及理论，总有一些纸上得来终觉浅的感觉。很多时候，道理说的头头是道，可是真正要上手的时候还是会一脸懵逼。或者是勉强能够搞一搞，但是过程当中总会遇到这样或者那样各种意想不到的问题。一方面是我们动手实践的不够， 另一方面也是理解不够深入。

在这一节中，我们将动手实践朴素贝叶斯模型。


## 2.1 朴素贝叶斯与文本分类

一般来说，我们认为狭义的事件的结果应该是有限的，也就是说事件的结果应该是一个离散值而不是连续值。所以早期的贝叶斯模型，在引入高斯混合模型的思想之前，针对的也是离散值的样本（存疑，笔者推测）。所以我们先抛开连续特征的场景，先来看看在离散样本当中，朴素贝叶斯模型有哪些实际应用。

在机器学习广泛的应用场景当中，有一个非常经典的应用场景，它的样本一定是离散的，它就是**自然语言处理**（Natural Language Processing）。在语言当中，无论是什么语言，无论是一个语句或是一段文本，它的最小单位要么是一个单词，要么是一个字。这些单元都是离散的，所以天生和朴素贝叶斯模型非常契合。

我们这次做的模型针对的场景是垃圾邮件的识别，这应该是我们生活当中经常接触到的功能。现在的邮箱基本上都有识别垃圾邮件的功能，如果发现是垃圾邮件，往往会直接屏蔽，不会展示给用户。早期的垃圾邮件和垃圾短信识别的功能都是通过朴素贝叶斯实现的。

在这个实验当中，我们用的是**UCI的数据集**。UCI大学的机器学习数据集非常出名，许多教材和课本上都使用了他们的数据集来作为例子。我们可以直接通过网页下载他们的数据，UCI的数据集里的数据都是免费的。

>https://archive.ics.uci.edu/ml/datasets/sms+spam+collection

下载完成之后，我们先挑选其中几条来看看：

>ham Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...  
ham Ok lar... Joking wif u oni...  
spam Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's  
ham U dun say so early hor... U c already then say...

这份数据是以txt文件类型保存，每行文本的第一个单词表示文本的类别，其中ham表示正常，spam表示是垃圾邮件。

我们首先读取文件，将文件当中的内容先读取到list当中，方便我们后续的处理。

```python
def read_file(filename):
    file = open (filename,'r')
    content = []
    for line in file.readlines():
        content.append(line)
    return content
```

我们查看一下前三条数据：

![[e48057b86d2b117a71bf4905891d8d90.png]]

可以发现类别和正文之间通过\t (tab)分开了，我们可以直接通过python的split方法将类别和正文分开。其中类别也就是我们想要模型学习的结果，在有监督学习当中称为label。文本部分也就是模型做出预测的依据，称为**特征**。在文本分类场景当中，特征就是**文本信息**。

我们将label和文本分开：

```python
labels = []
data = []

for i in smsTxt:
    row = i.split('\t')
    if len(row) == 2:
        labels.append(row[0])
        data.append(row[1])
```

### **过滤标点符号**

将文本和label分开之后，我们就需要对文本进行处理了。在进行处理之前，我们先随便拿一条数据来查看一下，这里我们选择了第一条：

>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\n'

这是一条非常典型的未处理之前的文本，当中不仅大小写字母混用，并且还有一些特殊符号。所以文本处理的第一步就是把所有字母全部小写，以及去除标点符号。

说起来比较复杂，但只要使用正则表达式，我们可以很方便地实现：

```python
import re
# 只保留英文字母和数字
rule = re.compile("[^a-zA-Z\d ]")
for i in range(len(data)):
    data[i] = re.sub("[^a-zA-Z\d ]",'',data[i].lower())
```

最后得到的结果如下：

>go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat


这里正则表达式非常简单，就是只保留英文字母和数字以及空格，其余所有的内容全部过滤。我们在传入的时候做了大小写转换，会把所有的大写字母转成小写。到这里为止，所有的特殊字符就都处理掉了，接下来就可以进行**分词**了。

英文的分词很简单，我们直接根据空格split即可。如果是中文分词，可以使用一些第三方库完成，之前的文章里介绍过，这里就不赘述了

### **安装nltk**

在接下来的文本处理当中，我们需要用到一个叫做nltk的自然语言处理的工具库。当中集成了很多非常好用的NLP工具，和之前的工具库一样，我们可以直接使用pip进行安装：

```python
pip3 install nltk
```

这里强烈建议使用Python3，因为Python2已经不再维护了。这步结束之后，只是装好了nltk库，nltk当中还有很多其他的资源文件需要我们下载。我们可以直接通过python进行下载：

```python
import nltk

nltk.download()
```

调用这个代码之后会弹出一个下载窗口

![[6813a69fb8998932959e1beaf3bf9410.jpg]]


我们全选然后点击下载即可，不过这个数据源在国外，在国内直接下载可能会很慢。除了科学上网之外，另一种方法是可以直接在github里下载对应的资源数据：https://github.com/nltk/nltk_data

需要注意的是，必须要把数据放在指定的位置，具体的安装位置可以调用一下download方法之后查看红框中的路径。或者也可以使用清华大学的镜像源，使用命令：

```python
pip3 install -i https://pypi.tuna.tsinghua.edu.cn/simple/nltk
```

下载好了之后，我们在Python当中执行：

```python
fron nltk.book import *
```

如果出现以下结果，就说明已经安装完毕：

![[6cf9df48704899a39acd92a442d3390f.jpg]]

### **去除停用词**

装好了nltk之后，我们要做的第一个预处理是去除停用词。

停用词英文是stop words，指的是文本当中对语义无关紧要的词汇。包含了常见的虚词、助词、介词等等。这些词语大部分只是修饰作用，对文本的语义内容起不到决定作用。因此在NLP领域当中，可以将其过滤，从而减少计算量提升模型精度。

Nltk当中为常见的主流语言提供了停用词表（不包括中文），我们传入指定的语言，将会返回一个停用词的list。我们在分词之后根据停用词表进行过滤即可。

![[378107448d10aedffbc8ad463a06129f.jpg]]

我们可以打印出所有英文的停用词看一下，大部分都是一些虚词和助词，可能出现在所有语境当中，对我们对文本进行分类几乎没有帮助。

### **词性归一化**

众所周知，英文当中的单词有很多形态。比如名词有单复数形式，有些特殊的名词复数形式还很不一样。动词有过去、现在以及未来三种时态，再加上完成时和第三人称一般时等，又有很多变化。

举例来说，do这个动词在文本当中会衍生出许多小词来。比如does, did, done, doing等，这些单词虽然各不相同，但是表示的意思完全一样。因此，在做英文NLP模型的时候，需要将这些时态的单词都还原成最基本的时态，这被称为是词性归一化。

原本这是一项非常复杂的工作，但我们有了nltk之后，这个工作变得简单了很多。要做单词归一化，我们需要用到nltk当中的两个工具。

第一个方法叫做pos_tag， 它接收一个单词的list作为入参。返回也是一个tuple的list，每个tuple当中包含两个值，一个是单词本身，第二个参数就是我们想要的词性。

举个例子：

![[b37bc368daf7adb2157fadcc6a115e2b.png]]

我们传入只有一个单词apple的list，在返回的结果当中除了apple之外，还多了一个NN，它表示apple是一个名词nouns。

关于返回的词性解释，感兴趣的可以自行查看官方文档的说明。

我们这里并不需要区分那么细，只需要区分最常用的动词、名词、形容词、副词就基本上够了。

我们可以直接根据返回结果的首字母做个简单的映射：

```python
from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer

# 获取单词的词性
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return None
```

通过pos_tag方法我们很容易就可以拿到单词的词性，但是这还不够，我们还需要将它还原成最基础的形态。这个时候需要用到另一个工具：WordNetLemmatizer

它的用途是根据单词以及单词的词性返回单词最一般的形态，也就是归一化的操作。

举个例子：

![[b99671322c677026842f0241fa517237.png]]

我们传入了box的复数形式：boxes，以及box对应的名词，它返回的结果正是我们想要的box。

我们结合刚刚实现的查询单词词性的方法，就可以完成单词的归一化了。

到这里为止，关于文本的初始化就算是差不多结束了。除了刚刚提到的内容之外，nltk还包含许多其他非常方便好用的工具库。由于篇幅的限制，我们不能一一穷尽，感兴趣的读者可以自行钻研，相信一定会很有收获。

下面，我们把刚才介绍的几种文本预处理的方法一起用上，对所有的短信进行预处理：

```python
for i in range(len(data)):
    data[i] = re.sub("[^a-zA-Z ]",'',data[i].lower())
    tokens = data[i].split(' ')
    tagged_sent=pos_tag([i for i in tokens if i and not i in stopwords.words('english')])
    wnl=WordNetLemmatizer()
    lemmas_sent= []
    for tag in tagged_sent:
        wordnet_pos=get_wordnet_pos(tag[1]) or wordnet.NOUN
        lemmas_sent.append(wnl.lemmatize(tag[0],pos=wordnet_pos))
        data[i]=lemmas_sent    
```

通过nltk的工具库，我们只需要几行代码，就可以完成文本的分词、停用词的过滤以及词性的归一化等工作。

接下来，我们就可以进行朴素贝叶斯的模型的训练与预测了。

首先，我们需要求出背景概率。所谓的背景概率，也就是指在不考虑任何特征的情况下，这份样本中信息当中天然的垃圾短信的概率。

这个其实很简单，我们只需要分别其实正常的邮件与垃圾邮件的数量然后分别除以总数即可：

```python
def base_prob(labels):
    pos,neg=0.0,0.0
    for i in labels:
        if i == 'ham':
            neg+=1
        else:
            pos+=1
    return pos/(pos+neg),neg/(pos+neg)
```

我们run一下测试一下结果：

![[188a2b294a12b31b5606afa9610ee0bb.png]]

可以看到垃圾短信的概率只占13%，大部分短信都是正常的。这也符合我们的生活经验，毕竟垃圾短信是少数。

接下来我们需要求出每个单词属于各个类别的概率，也就是要求一个单词的概率表。这段代码稍微复杂一些，但是也不麻烦：

```python
def word_prob(data,labels):
    n=len(data)
    # 创建词表
    word_dict={}
    for i in range(n):
        lab= labels[i]
        # 先转set再转list，去除重复的常规操作
        dat=list(set(data[i]))
        for word in dat:
            # 单词不在dict中的时候创建dict，默认从1开始计数，为了防止除0
            if word not in word_dict:
                word_dict[word] = {'ham' : 1, 'spam': 1} # 拉普帕斯平滑避免除0
            word_dict[word][lab] += 1
    # 将数量转化成概率
    for i in word_dict:
        dt = word_dict[i]
        ham = dt['ham']
        spam = dt['spam']
        word_dict[i]['ham'] = ham / float(ham + spam)
        word_dict[i]['spam'] = spam / float(ham + spam)
    return word_dict
```

同样，我们运行一下测试一下结果：

![[3bf0c5c5cb91f0865c40c7d6206a4fbb.jpg]]

这些都有了之后，就是预测的重头戏了。这里有一点需要注意，根据我们上文当中的公式，我们在预测文本的概率的时候，会用到多个概率的连乘。由于浮点数有精度限制，所以我们不能直接计算乘积，而是要将它转化成对数，这样我们就可以通过加法来代替乘法，就可以避免连乘带来的精度问题了。

```python
import math

def predict(samples,word_prob,base_p,base_n):
    ret = []
    for sam in samples:
        neg = math.log(base_n)
        pos = math.log(base_p)
        for word in sam:
            if word not in word_prob:
                continue
            neg += math.log(word_prob[word]['spam'])
            pos += math.log(word_prob[word]['ham'])
        ret.append('ham' if pos > neg else 'spam')
    return ret
```


预测的方法也非常简单，我们分别计算出一个文本属于spam以及ham的概率，然后选择概率较大的那个作为最终的结果即可。

我们将原始数据分隔成训练集以及预测集，调用我们刚刚编写的算法获取预测的结果：

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.25)
base_p, base_n = base_prob(y_train)
word_dt = word_prob(x_train, y_train)
ret = predict(x_test, word_dt, base_p, base_n)
```

最后，我们调用一下sklearn当中的classification_report方法来获取贝叶斯模型的预测效果：

![[3a154b495363dbba46d9c02e2fec25a9.png]]

从上图当中看，贝叶斯模型的预测效果还是不错的。对于垃圾文本识别的准确率有90%，可惜的是召回率低了一点，说明有一些比较模糊的垃圾文本没有识别出来。这也是目前这个场景下问题的难点之一，但总的来说，贝叶斯模型的原理虽然简单，但是效果不错，也正因此，时至今日，它依旧还在发挥着用处。

NLP是当今机器学习领域非常复杂和困难的应用场景之一，关于文本的预处理以及模型的选择和优化都存在着大量的操作。本文当中列举的只是其中最简单也是最常用的部分。

到这里，关于朴素贝叶斯的实践就结束了。我想亲手从零开始写出一个可以用的模型，一定是一件非常让人兴奋的事情。但关于朴素贝叶斯模型其实还没有结束，它仍然有许多细节等待着大家去思考，也有很多引申的知识。模型虽然简单，但仍然值得我们用心地体会。