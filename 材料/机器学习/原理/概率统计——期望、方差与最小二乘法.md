
# 概率统计——期望、方差与最小二乘法


## 1.1 期望

期望这个概念我们很早就在课本里接触了，维基百科的定义是：它表示的是**一个随机变量的值在每次实验当中可能出现的结果乘上结果概率的总和**。换句话说，期望值衡量的是多次实验下，所有可能得到的状态的平均结果。

我们举两个简单的例子，第一个例子是掷骰子。

我们都知道一个骰子有6个面，分别是1，2，3，4，5，6。我们每次投掷得到其中每一个面朝上的概率都是一样的，是1/6。对于投骰子这个事件而言，它的期望应该是:

$$E(X)=1*\frac{1}{6}+2*\frac{2}{6}+...+6*\frac{1}{6}=3.5$$
也就是说，我们如果投掷大量的骰子，得到的平均结果应该是3.5，但是骰子上并没有这个点数可以被掷出来。


另一个经典的例子就是**博弈游戏**，老赌徒们水平各有高低，但一定深谙期望这个概念。举个最简单的例子，比如美国轮盘当中一个有38个数字，每次可以押一个数字。如果押中了，赌徒可以获得35倍的奖金，如果押不中，钱打水漂。我们来算下期望：

$$E(X)=-1*\frac{37}{38}+35*\frac{1}{38}=-\frac{3}{38}$$
我们可以发现这个期望是一个**负值**，也就是说短期内可能是盈利的，如果我们多次游戏，必输无疑。

## 1.2 方差

第二个概念是方差，方差衡量的是**变量的离散程度**。它的公式是：

$$V(X)=E((X-\mu)^2)$$

这里的μ指的是就是**变量X的期望值**。也就是说，方差指的是变量X与它期望值平方差的期望值，**方差越大，表示X变量离散化越严重，越小，说明X波动范围越小**。

由于 $(X-\mu)^2$  一定是一个非负值，所以变量的方差一定是非负的。我们同样用赌博举个例子，假设我们现在有一个抛硬币的游戏。每次抛一枚硬币，如果正面朝上则赢10000元，如果背面朝上，则输9000元。我们很容易看出来，这个游戏的期望是500元。也就是说我们平均每轮能赢500元。

但是，我们不用算就可以看出来这个游戏的方差很大。如果我们真的去玩这个游戏，大概率会在赢得很多和输得很惨之间徘徊，很难稳定盈利。也有可能我们还没有来得及赢钱就破产了。

通过方差这个概念，我们很容易理解为什么在游戏当中，**倍押策略**不可行。

所谓的倍押策略是指，**在一个50%赢率的游戏当中，我们当前如果输了钱，那么下一轮则倍押当前输的钱**。如果还输了继续倍押，直到赢为止。通过这种策略呢，可以抵抗连续输的风险，理论上来说只要最终赢一把，就可以赢回之前所有的钱。

我们了解了方差的概念之后，很容易发现这个策略是不可行的，因为这种策略的方差非常大。在盈利之前，很容易震荡到一个不可能承受的值，也就是会出现倾家荡产也不够押下一把的情况。

## 1.3 标准差

理解了方差，标准差也就很好理解。标准差就是方差的平方根，和标准差一样，同样用来反映标本的离散情况。

由于方差和标准差的定义和使用情况非常类似，所以一般情况下，我们使用方差的场景会更多。所以这里不过多介绍，知道概念和计算方法即可。


## 1.4 最小二乘法

最小二乘法非常出名，现在机器学习和深度学习很多模型都广泛使用。所谓的二乘，其实就是**平方**的意思。也被称为**最小平方法**，是一种用来**评估预测结果与实际误差**的方法。

最小我们很容易理解，这里的平方是什么呢？

平方指的是**误差的平方**，我们写出公式，就很容易明白了：

$$SE=\sum(y_{pred}-y)^2$$
这里的 $y_{pred}$ 指的是预测值，而 $y$ 指的是样本值。从公式我们可以看出来，其平方误差就是所有样本预测值与真实值误差的平方和。最小二乘法就是优化这个平法误差，使得它尽可能小，来寻找最佳的 $y_{pred}$ 的方法。

这个方法主要用在回归模型当中。

我们简单介绍一下回归模型的概念，在机器学习领域，最常用的模型可以分为**回归模型**与**分类模型**。这两者的差别就在于模型预测的结果不同，在分类模型当中，模型的预测结果是样本所属的类别。而回归模型，模型的预测结果则是一个具体的值。

举个简单的例子，比如我今天要设计一个模型预测明天股票是涨是跌，显然股票要么涨，要么跌，只有两种情况，所以这是一个分类模型，但如果我要预测明天股票的具体指数，那么它的结果是一个具体的值，这个就是回归模型。

我们通常使用平方差来反应回归模型的预测能力，我们通过减少误差，提升模型的能力，达到更加精确的效果。问题来了，我们减少误差，为什么减少误差就能提升模型的能力呢？

首先，虽然我们将模型的预测结果简写为 $y_{pred}$, 这个 $y_{pred}$ 不是天上掉下来的，它背后是模型通过一些参数以及自变量 $x$  计算出来的。举个最简单的例子，如果我们把一个一元一次函数看成是一个回归模型，那么方程可以写成：

$$y_{pred}= wx+b$$

这里的 $w$ 和 $b$ 就是参数。

我们减小模型的平方误差，也就是找到更好的 $w$ 和 $b$ ,使得它计算得到的 $y_{pred}$ 更加精确，误差更小。

那么我们怎么减少误差呢？

我们先来观察一下误差平方和的公式，可以发现，它是一个二次函数。我们高中的时候就曾经学过，二次函数求极值，可以通过**求导**得到。除了求导之外，还有一些其他的最优化方法，这些不是本文的重点，会在以后介绍**线性回归模型**文章和大家分享。

最后，我们再回顾一下最小平方和和方差的公式，不知道大家有没有什么感觉。如果我们把样本真实的结果看成是期望值，那么误差的平方和不就和方差一样了吗？


我个人认为是可以这么理解的，就好像方差衡量的是样本针对期望值的离散程度一样，误差平方和反应的是**预测结果针对真实值的离散情况**。自然预测结果在真实值离散程度越低，模型的效果越好。所以这两个概念的本质是相通的。

